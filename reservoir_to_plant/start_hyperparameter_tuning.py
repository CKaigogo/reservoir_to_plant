import os
import sys
import torch
import wandb
from pathlib import Path

os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'

os.environ['WANDB_START_METHOD'] = 'thread'
os.environ['WANDB_INIT_TIMEOUT'] = '300'
os.environ['WANDB_SILENT'] = 'true'

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent.resolve()
sys.path.append(str(current_dir))
if not (current_dir / "onpolicy").exists():
    sys.path.append(str(current_dir.parent))

from training_config import get_config

# å¯¼å…¥è®­ç»ƒè„šæœ¬
from train_water import train

def safe_wandb_init(max_retries=3, timeout=120):
    """å®‰å…¨çš„W&Båˆå§‹åŒ–"""
    for attempt in range(max_retries):
        try:
            print(f"å°è¯•åˆå§‹åŒ–W&B (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)")

            # ä½¿ç”¨æ›´ä¿å®ˆçš„è®¾ç½®
            run = wandb.init(
                settings=wandb.Settings(
                    init_timeout=timeout,
                    start_method='thread',
                    _disable_stats=True,
                    _disable_meta=True,
                    console='off'
                )
            )

            print(f" W&Båˆå§‹åŒ–æˆåŠŸ: {run.name}")
            return run

        except Exception as e:
            print(f" W&Båˆå§‹åŒ–å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                print(f" ç­‰å¾… {5 * (attempt + 1)} ç§’åé‡è¯•...")
                import time
                time.sleep(5 * (attempt + 1))
            else:
                print(" æ‰€æœ‰W&Båˆå§‹åŒ–å°è¯•å¤±è´¥ï¼Œå°†ä»¥ç¦»çº¿æ¨¡å¼ç»§ç»­")
                return None

def main():
    """ä¿®å¤ç‰ˆä¸»è®­ç»ƒå‡½æ•°"""
    print("å¯åŠ¨è¶…å‚æ•°è°ƒä¼˜...")

    run = safe_wandb_init()

    if run is None:
        print(" W&Båˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®ç»§ç»­è®­ç»ƒ")
        # åˆ›å»ºé»˜è®¤é…ç½®
        parser = get_improved_config()
        all_args = parser.parse_args([])
        all_args.exp_name = "fallback_training"
        all_args.use_wandb = False
        wandb_config = {}
    else:
        # è·å–åŸºç¡€é…ç½®
        print("--- Loading base config from get_improved_config() ---")
        parser = get_improved_config()
        all_args = parser.parse_args([])

        # ç”¨W&Bå‚æ•°è¦†ç›–
        print("--- Overriding args with W&B sweep config ---")
        wandb_config = dict(wandb.config)

        for key, value in wandb_config.items():
            if hasattr(all_args, key):
                print(f"  - Overriding '{key}': from '{getattr(all_args, key)}' to '{value}'")
                setattr(all_args, key, value)

        all_args.exp_name = run.name
        all_args.use_wandb = True

    # è®¾ç½®å¿…éœ€çš„ç¯å¢ƒå‚æ•°
    print("--- Setting fixed, non-tuned parameters ---")
    all_args.use_fixed_connections = True
    all_args.continuous_management = True
    all_args.env_name = "WaterManagement"

    # å¯ç”¨ç®€åŒ–åŠ¨ä½œç©ºé—´
    all_args.use_simplified_actions = True
    print(f"  - Set 'use_simplified_actions': True")

    # æ·»åŠ æ–°çš„å¿…éœ€å‚æ•°
    if not hasattr(all_args, 'num_agents'):
        all_args.num_agents = all_args.num_reservoirs + all_args.num_plants

    print(f"  - Set 'num_agents': {all_args.num_agents}")
    print(f"  - Set 'continuous_management': {all_args.continuous_management}")

    # è®¾ç½®ç›®å½•å’Œè®¾å¤‡
    if run:
        run_dir = Path(run.dir)
    else:
        run_dir = Path.cwd() / "fallback_results"
        run_dir.mkdir(exist_ok=True)

    all_args.run_dir = run_dir
    all_args.log_dir = run_dir / "logs"
    all_args.model_dir = run_dir / "models"
    all_args.log_dir.mkdir(parents=True, exist_ok=True)
    all_args.model_dir.mkdir(parents=True, exist_ok=True)

    # ğŸ”§ ä¿®å¤5: æ›´å®‰å…¨çš„è®¾å¤‡é€‰æ‹©
    if torch.cuda.is_available() and all_args.cuda:
        device = torch.device("cuda:0")
        print(f" ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
        # è®¾ç½®CUDAä¼˜åŒ–å‚æ•°
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False

        # ğŸ”§ æ–°å¢ï¼šæ¸…ç†CUDAç¼“å­˜
        torch.cuda.empty_cache()
        print(f" å·²æ¸…ç†CUDAç¼“å­˜")
    else:
        device = torch.device("cpu")
        print("ä½¿ç”¨CPU")

    all_args.device = device

    print("=" * 60)
    print(f" Final Configuration for Run: {all_args.exp_name}")
    print(f"  - Device: {device}")
    print(f"  - Environment: {all_args.num_reservoirs} reservoirs, {all_args.num_plants} plants")
    print(f"  - Continuous Management: {all_args.continuous_management}")
    print(f"  - W&B Enabled: {all_args.use_wandb}")
    if wandb_config:
        print(f"  - Sweep Config: {wandb_config}")
    print("=" * 60)

    # å¸¦å¼‚å¸¸å¤„ç†çš„è®­ç»ƒå¯åŠ¨
    print("\n Starting training with comprehensive error handling...")


    training_success = False
    error_message = ""

    try:
        train(all_args)
        training_success = True
        print("\n Training completed successfully!")

    except KeyboardInterrupt:
        print("\n Training interrupted by user")
        error_message = "User interruption"

    except RuntimeError as e:
        if "CUDA" in str(e) or "cuda" in str(e):
            print(f"\n CUDAé”™è¯¯: {e}")
            print(" å°è¯•å‡å°‘batch_sizeæˆ–ä½¿ç”¨CPUè®­ç»ƒ")
            error_message = f"CUDA error: {e}"
        elif "tensor" in str(e).lower() and "device" in str(e).lower():
            print(f"\n Tensorè®¾å¤‡é”™è¯¯: {e}")
            print("å»ºè®®: æ£€æŸ¥tensorè®¾å¤‡ä¸€è‡´æ€§")
            error_message = f"Tensor device error: {e}"
        else:
            print(f"\n Runtimeé”™è¯¯: {e}")
            error_message = f"Runtime error: {e}"

    except Exception as e:
        print(f"\n Training error: {e}")
        import traceback
        traceback.print_exc()
        error_message = str(e)

    finally:
        # å®‰å…¨çš„W&Bæ¸…ç†
        try:
            if run and all_args.use_wandb:
                if not training_success:
                    wandb.log({"training_success": False, "error_message": error_message})
                else:
                    wandb.log({"training_success": True})

                print("æ­£åœ¨åŒæ­¥W&Bæ•°æ®...")
                run.finish()
                print("W&Bæ•°æ®åŒæ­¥å®Œæˆ")

        except Exception as cleanup_error:
            print(f" W&Bæ¸…ç†æ—¶å‡ºé”™: {cleanup_error}")

        # æœ€ç»ˆCUDAæ¸…ç†
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(" å·²æ¸…ç†CUDAç¼“å­˜")

        print(f"\n è®­ç»ƒç»“æœ: {'æˆåŠŸ' if training_success else 'å¤±è´¥'}")
        if not training_success:
            print(f" é”™è¯¯ä¿¡æ¯: {error_message}")


if __name__ == "__main__":
    main()