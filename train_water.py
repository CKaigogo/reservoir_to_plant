"""
æ°´èµ„æºç®¡ç†è®­ç»ƒè„šæœ¬
ä¸ wandb agent å…¼å®¹
"""

import os
import sys
import torch
import numpy as np
import math
from datetime import datetime
from pathlib import Path
import matplotlib.pyplot as plt
import json
import wandb
from gym import spaces
from scipy.signal import savgol_filter
import seaborn as sns
import importlib.util
import time
import gc
from collections import deque
import threading
import argparse
import psutil


os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'

#  æ·»åŠ WandBä¼˜åŒ–è®¾ç½®
os.environ['WANDB_START_METHOD'] = 'thread'
os.environ['WANDB_INIT_TIMEOUT'] = '300'
os.environ['WANDB_SILENT'] = 'true'

current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
sys.path.append(str(project_root))


from training_config import get_config, get_optimized_hyperparameters, print_config_summary
CONFIG_AVAILABLE = True


# å¯¼å…¥wandbè®°å½•å™¨
try:
    from wandb_data_extractor import EnhancedWandBLogger
    ENHANCED_LOGGER_AVAILABLE = True
    print(" WandBè®°å½•å™¨å·²å¯¼å…¥")
except ImportError:
    ENHANCED_LOGGER_AVAILABLE = False
    print(" WandBè®°å½•å™¨ä¸å¯ç”¨")


from onpolicy.algorithms.r_mappo.r_mappo import R_MAPPO
from onpolicy.algorithms.r_mappo.algorithm.rMAPPOPolicy import R_MAPPOPolicy
from onpolicy.utils.shared_buffer import SharedReplayBuffer
from onpolicy.utils.util import update_linear_schedule


env_file_path = project_root / 'history' / 'MAPPO' / 'onpolicy' / 'envs' / 'water_env' / 'WaterManagementEnv.py'
spec = importlib.util.spec_from_file_location("WaterManagementEnv", env_file_path)
water_env_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(water_env_module)
WaterManagementEnv = water_env_module.WaterManagementEnv
print(f" å·²ä»ä»¥ä¸‹è·¯å¾„åŠ è½½ç¯å¢ƒ: {env_file_path}")

class SeparatedRewardLogger:
    """è®°å½•ä¸åŒç±»å‹å¥–åŠ±çš„æ—¥å¿—è®°å½•å™¨"""

    def __init__(self, episode_length=96):
        self.episode_length = episode_length
        self.step_rewards = []
        self.episode_rewards = []
        self.component_rewards = {
            'supply': [],
            'safety': [],
            'ecological': [],
            'stability': [],
            'shaping': []
        }
        self.agent_rewards = {}

    def log_step_reward(self, step_reward, components=None, agent_rewards=None):
        """è®°å½•å•æ­¥å¥–åŠ±"""
        self.step_rewards.append(step_reward)

        if components:
            for component, value in components.items():
                if component in self.component_rewards:
                    self.component_rewards[component].append(value)

        if agent_rewards:
            for agent_id, reward in agent_rewards.items():
                if agent_id not in self.agent_rewards:
                    self.agent_rewards[agent_id] = []
                self.agent_rewards[agent_id].append(reward)

    def log_episode_reward(self, episode_reward):
        """è®°å½•Episodeå¥–åŠ±"""
        self.episode_rewards.append(episode_reward)

    def get_separated_metrics(self):
        """è·å–åˆ†ç¦»çš„æŒ‡æ ‡"""
        if not self.step_rewards:
            return {}

        metrics = {
            # åŸºç¡€æŒ‡æ ‡
            'step_reward_mean': np.mean(self.step_rewards),
            'step_reward_std': np.std(self.step_rewards),
            'episode_reward_total': sum(self.step_rewards),
            'episode_reward_normalized': np.mean(self.step_rewards),
            'daily_reward': sum(self.step_rewards) / (len(self.step_rewards) / 24.0),
            'hourly_reward': np.mean(self.step_rewards),

            # ç»„ä»¶æŒ‡æ ‡
            'components': {},
            'agents': {}
        }

        for component, rewards in self.component_rewards.items():
            if rewards:
                metrics['components'][component] = {
                    'mean': np.mean(rewards),
                    'total': sum(rewards),
                    'contribution': sum(rewards) / sum(self.step_rewards) if sum(self.step_rewards) != 0 else 0
                }

        # æ™ºèƒ½ä½“å¥–åŠ±
        for agent_id, rewards in self.agent_rewards.items():
            if rewards:
                metrics['agents'][agent_id] = {
                    'mean': np.mean(rewards),
                    'total': sum(rewards),
                    'std': np.std(rewards)
                }

        return metrics

    def reset(self):
        """é‡ç½®è®°å½•å™¨"""
        self.step_rewards = []
        self.component_rewards = {k: [] for k in self.component_rewards}
        self.agent_rewards = {}


def safe_tensor_to_numpy(tensor, device_type='cpu'):
    if isinstance(tensor, torch.Tensor):
        if tensor.is_cuda:
            return tensor.cpu().detach().numpy()
        else:
            return tensor.detach().numpy()
    elif isinstance(tensor, np.ndarray):
        return tensor
    else:
        return np.array(tensor)


def safe_numpy_to_tensor(array, device, dtype=torch.float32):
    if isinstance(array, torch.Tensor):
        return array.to(device)
    else:
        return torch.from_numpy(np.array(array, dtype=dtype.numpy())).to(device)


def make_train_env(args):
    """ç¯å¢ƒåˆ›å»ºå‡½æ•°"""
    try:
        env = WaterManagementEnv(
            num_reservoirs=args.num_reservoirs,
            num_plants=args.num_plants,
            use_fixed_connections=args.use_fixed_connections,
            continuous_management=args.continuous_management,
            max_episode_steps=args.episode_length,
            enable_optimizations=getattr(args, 'enable_optimizations', True)
        )

        if getattr(args, 'enable_optimizations', False):
            if hasattr(env, 'enable_training_optimizations'):
                env.enable_training_optimizations()
                print("åŠ¨æ€å¯ç”¨è®­ç»ƒä¼˜åŒ–åŠŸèƒ½")

            # è·å–ä¼˜åŒ–çŠ¶æ€
            if hasattr(env, 'get_optimization_status'):
                status = env.get_optimization_status()
                print(f"   - å¥–åŠ±ç³»ç»Ÿ: {status.get('reward_system_type', 'Unknown')}")
                print(f"   - è§‚æµ‹ç®¡ç†å™¨: {status.get('obs_manager_type', 'Unknown')}")
                print(f"   - è§‚æµ‹ç»´åº¦: {status.get('obs_dimension', 'Unknown')}")

        print(f"è®­ç»ƒç¯å¢ƒåˆ›å»ºæˆåŠŸ: {len(env.agents)} ä¸ªæ™ºèƒ½ä½“")

        if not hasattr(env, 'observation_space'):
            raise AttributeError("ç¯å¢ƒç¼ºå°‘observation_spaceå±æ€§")
        if not hasattr(env, 'agents'):
            raise AttributeError("ç¯å¢ƒç¼ºå°‘agentså±æ€§")

        return env

    except Exception as e:
        print(f"ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise


def convert_action_space_for_mappo(env):
    """åŠ¨ä½œç©ºé—´çš„è½¬æ¢"""
    action_dims = []

    # æ˜¯å¦ä½¿ç”¨ç®€åŒ–åŠ¨ä½œç©ºé—´
    use_simplified = getattr(env, 'use_simplified_actions', False)

    if use_simplified:
        for agent_id in env.agents:
            action_dims.append(2)
        max_action_dim = 2
    else:
        for agent_id in env.agents:
            if agent_id.startswith('reservoir_'):
                action_dims.append(7)
            else:
                action_dims.append(3)
        max_action_dim = max(action_dims)

    print(f"åŠ¨ä½œç©ºé—´ç»Ÿä¸€ï¼šæœ€å¤§ç»´åº¦ {max_action_dim}")

    action_spaces = []
    for agent_id in env.agents:
        action_spaces.append(spaces.Box(low=0.0, high=1.0, shape=(max_action_dim,), dtype=np.float32))

    return action_spaces


def init_hierarchical_policy(env, args):
    """é’ˆå¯¹å¤§è§„æ¨¡ç³»ç»Ÿçš„ç­–ç•¥åˆå§‹åŒ–"""
    obs_space = env.observation_space
    act_space_list = convert_action_space_for_mappo(env)

    first_agent = list(env.agents)[0]
    obs_dim = obs_space[first_agent].shape[0]
    num_agents = len(env.agents)

    print(f" å¤§è§„æ¨¡ç­–ç•¥åˆå§‹åŒ–: {num_agents}ä¸ªæ™ºèƒ½ä½“ï¼Œè§‚å¯Ÿç»´åº¦: {obs_dim}")

    # å¤§è§„æ¨¡agentä¸‹çš„ç½‘ç»œæ¶æ„è°ƒæ•´
    if num_agents >= 15:
        args.hidden_size = 128 if args.enable_optimizations else 256
        args.layer_N = 3
        args.entropy_coef = 0.02  # å¢åŠ æ¢ç´¢
        args.lr = 3e-5 if args.enable_optimizations else 4e-5  # é™ä½å­¦ä¹ ç‡
        args.critic_lr = 1e-4
        print("å¤§è§„æ¨¡ä¸‹ä¼˜åŒ–é…ç½®")

    print(f"   - Hidden size: {args.hidden_size}")
    print(f"   - Layers: {args.layer_N}")
    print(f"   - Learning rates: actor={args.lr}, critic={args.critic_lr}")
    print(f"   - Entropy coef: {args.entropy_coef}")

    unified_obs_space = obs_space[first_agent]
    unified_action_space = act_space_list[0]

    device = args.device
    policy = R_MAPPOPolicy(args, unified_obs_space, unified_obs_space, unified_action_space, device=device)
    trainer = R_MAPPO(args, policy, device=device)

    print(f" ç­–ç•¥ç½‘ç»œåˆå§‹åŒ–å®Œæˆ: {num_agents}ä¸ªæ™ºèƒ½ä½“")
    return policy, trainer


def convert_actions_from_mappo(env, mappo_actions):

    if hasattr(env, 'use_simplified_actions') and env.use_simplified_actions:
        simplified_actions = env.simplified_action_processor.convert_mappo_actions_to_simplified(
            mappo_actions, env.agents
        )
        original_actions = env.simplified_action_processor.convert_simplified_to_original_actions(
            simplified_actions
        )
        return original_actions
    else:
        actions = {}
        num_plants = getattr(env, 'num_plants', 1)  # è·å–å®é™…æ°´å‚æ•°é‡

        for i, agent_id in enumerate(env.agents):
            agent_action = mappo_actions[i]

            if agent_id.startswith('reservoir_'):
                actions[agent_id] = {
                    'total_release_ratio': np.array([np.clip(agent_action[0], 0.0, 1.0)]),
                    'allocation_weights': np.clip(agent_action[1:1 + num_plants], 0.0, 1.0),
                    'emergency_release': int(np.round(np.clip(agent_action[1 + num_plants], 0, 1)))
                }
            else:
                actions[agent_id] = {
                    'demand_adjustment': np.array([np.clip(agent_action[0] * 1.0 + 1.0, 0.5, 1.5)]),
                    'priority_level': int(np.round(np.clip(agent_action[1] * 2, 0, 2))),
                    'storage_strategy': np.array([np.clip(agent_action[2], 0.0, 1.0)])
                }

        return actions


class EnhancedTrainingMonitor:
    """ç”¨äºè®­ç»ƒç›‘æ§"""

    def __init__(self, log_interval=10):
        self.log_interval = log_interval

        # ç›‘æ§æ•°æ®
        self.metrics = {
            'episode_rewards': deque(maxlen=1000),
            'episode_lengths': deque(maxlen=1000),
            'training_steps': deque(maxlen=1000),
            'eval_rewards': deque(maxlen=100),
            'system_metrics': deque(maxlen=1000)
        }
        self.start_time = time.time()

    def connect_environment(self, env):
        """ä»…ç”¨äºå…¼å®¹æ€§ï¼Œä¸æ‰§è¡Œä»»ä½•æ“ä½œ"""
        pass

    def log_episode(self, episode, episode_reward, episode_steps, total_steps, infos=None):
        """è®°å½•episodeä¿¡æ¯"""
        self.metrics['episode_rewards'].append(episode_reward)
        self.metrics['episode_lengths'].append(episode_steps)
        self.metrics['training_steps'].append(total_steps)

        try:
            import wandb
            if wandb.run is not None:
                # ç«‹å³è®°å½•åˆ° wandb
                episode_time = time.time() - self.start_time
                log_data = {
                    "monitor/episode": int(episode),
                    "monitor/episode_reward": float(episode_reward),
                    "monitor/episode_steps": int(episode_steps),
                    "monitor/total_steps": int(total_steps),
                    "monitor/training_time": float(episode_time),
                    "global_step": int(total_steps)
                }

                # æ·»åŠ è¯¦ç»†ä¿¡æ¯
                if infos:
                    first_agent_info = list(infos.values())[0]
                    if isinstance(first_agent_info, dict):
                        if 'phase' in first_agent_info:
                            log_data["monitor/phase"] = str(first_agent_info['phase'])
                        if 'exploration_active' in first_agent_info:
                            log_data["monitor/exploration_active"] = bool(first_agent_info['exploration_active'])

                wandb.log(log_data)
                print(f" Monitorå¼ºåˆ¶è®°å½•: Episode {episode}")
        except Exception as e:
            print(f" Monitor wandb è®°å½•å¤±è´¥: {e}")


        if episode % self.log_interval == 0:
            avg_reward = np.mean(list(self.metrics['episode_rewards'])[-10:])
            avg_length = np.mean(list(self.metrics['episode_lengths'])[-10:])
            elapsed_time = time.time() - self.start_time

            print(f"ğŸ“Š Episode {episode}: Reward={episode_reward:.2f}, "
                  f"AvgReward(10)={avg_reward:.2f}, Length={episode_steps}, "
                  f"Steps={total_steps}, Time={elapsed_time:.1f}s")

        try:
            import wandb
            if wandb.run is not None:
                monitor_log_data = {
                    "monitor/episode": int(episode),
                    "monitor/episode_reward": float(episode_reward),
                    "monitor/episode_steps": int(episode_steps),
                    "monitor/total_steps": int(total_steps),
                    "monitor/timestamp": time.time()
                }

                if infos:
                    for agent_id, info in infos.items():
                        if isinstance(info, dict):
                            if 'phase' in info:
                                monitor_log_data["monitor/current_phase"] = str(info['phase'])
                            break

                wandb.log(monitor_log_data)
                print(f"Monitor wandb è®°å½•: Episode {episode}")
        except Exception as e:
            print(f" Monitor wandb è®°å½•å¤±è´¥: {e}")

    def log_eval(self, eval_reward, eval_metrics=None):
        """è®°å½•è¯„ä¼°ä¿¡æ¯"""
        self.metrics['eval_rewards'].append(eval_reward)
        print(f" è¯„ä¼°å¥–åŠ±: {eval_reward:.3f}")

    def log_enhanced_analysis(self, env):
        """åˆ†æä¿¡æ¯"""
        try:
            if hasattr(env, 'get_enhanced_reward_analysis'):
                analysis = env.get_enhanced_reward_analysis()

                if 'status' not in analysis:
                    exploration_summary = analysis.get('exploration_summary', {})
                    current_state = exploration_summary.get('current_state', {})

                    phase = current_state.get('phase', 'unknown')
                    effectiveness = current_state.get('exploration_effectiveness', 0.0)

                    # è®°å½•åˆ°ç³»ç»ŸæŒ‡æ ‡
                    self.metrics['system_metrics'].append({
                        'timestamp': time.time(),
                        'phase': phase,
                        'effectiveness': effectiveness,
                        'analysis': analysis
                    })

                    print(f"å¢å¼ºåˆ†æ: Phase={phase}, Effectiveness={effectiveness:.3f}")

                    return analysis
        except Exception as e:
            print(f" å¢å¼ºåˆ†æè®°å½•å¤±è´¥: {e}")

        return None

    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.metrics['episode_rewards']:
            return {}

        return {
            'avg_reward': np.mean(self.metrics['episode_rewards']),
            'std_reward': np.std(self.metrics['episode_rewards']),
            'max_reward': np.max(self.metrics['episode_rewards']),
            'avg_length': np.mean(self.metrics['episode_lengths']),
            'total_episodes': len(self.metrics['episode_rewards']),
        }

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        pass


def safe_wandb_log(data, step=None):
    """å®‰å…¨çš„W&Bè®°å½•å‡½æ•°"""
    try:
        if wandb.run is not None:
            if step is not None:
                wandb.log(data, step=step)
            else:
                wandb.log(data)
            return True
    except Exception as e:
        print(f" W&Bè®°å½•å¤±è´¥: {e}")
        return False


def initialize_wandb(args):
    """åˆå§‹åŒ–wandb"""
    if args.use_wandb:
        max_retries = 3
        retry_count = 0

        while retry_count < max_retries:
            try:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                project_suffix = "_optimized" if getattr(args, 'enable_optimizations', True) else "_standard"
                run_name = f"water_management{project_suffix}_{timestamp}"

                tags = ["water_management", "mappo", "multi_agent"]
                if getattr(args, 'enable_optimizations', False):
                    tags.extend(["optimized", "simplified_reward", "enhanced_exploration", "reduced_obs"])

                # é…ç½®
                config = {
                    'num_reservoirs': args.num_reservoirs,
                    'num_plants': args.num_plants,
                    'episode_length': args.episode_length,
                    'num_env_steps': args.num_env_steps,
                    'lr': args.lr,
                    'critic_lr': getattr(args, 'critic_lr', args.lr),
                    'hidden_size': args.hidden_size,
                    'ppo_epoch': args.ppo_epoch,
                    'entropy_coef': args.entropy_coef,
                    'clip_param': args.clip_param,
                    'enable_optimizations': getattr(args, 'enable_optimizations', False),
                    'continuous_management': args.continuous_management,
                    'algorithm_name': getattr(args, 'algorithm_name', 'mappo'),
                    'experiment_name': getattr(args, 'experiment_name', 'water_management'),
                    'seed': args.seed
                }

                project_name = getattr(args, 'wandb_project', 'uncategorized')

                print(f"å°è¯•åˆå§‹åŒ– WandB (å°è¯• {retry_count + 1}/{max_retries})...")

                wandb.init(
                    project=project_name,
                    name=run_name,
                    config=config,
                    group="optimized_training" if getattr(args, 'enable_optimizations', False) else "standard_training",
                    tags=tags,
                    settings=wandb.Settings(
                        init_timeout=300,
                        start_method='thread',
                        _disable_stats=True,
                        _disable_meta=True,
                        console='off',
                        # æ•°æ®ä¼˜åŒ–è®¾ç½®
                        _disable_code=True,
                        _disable_machine_info=True,
                        save_code=False,
                        log_interval=100,
                        _sync_run_history=False,
                        _sync_plots=False,
                        console_logging=False,
                        run_mode="online",
                        _disable_git=True,
                        _save_requirements=False
                    )
                )
                print(f" WandBåˆå§‹åŒ–æˆåŠŸ: {run_name}")
                print(f"é¡¹ç›®: {project_name}")
                print(f" æ ‡ç­¾: {', '.join(tags)}")
                return True

            except Exception as e:
                retry_count += 1
                print(f" WandBåˆå§‹åŒ–å¤±è´¥ (å°è¯• {retry_count}/{max_retries}): {e}")

                if retry_count < max_retries:
                    print(f"ç­‰å¾… {retry_count * 5} ç§’åé‡è¯•...")
                    time.sleep(retry_count * 5)
                else:
                    print("å°è¯•å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ç¦»çº¿æ¨¡å¼")

                    try:
                        wandb.init(
                            project=project_name,
                            name=run_name,
                            config=config,
                            group="optimized_training" if getattr(args, 'enable_optimizations',
                                                                  False) else "standard_training",
                            tags=tags,
                            mode="offline"
                        )
                        print(f"WandBç¦»çº¿æ¨¡å¼å¯åŠ¨æˆåŠŸ: {run_name}")
                        return True
                    except Exception as offline_e:
                        print(f"ç¦»çº¿æ¨¡å¼ä¹Ÿå¤±è´¥: {offline_e}")
                        print(f" å»ºè®®: 1) æ£€æŸ¥ç½‘ç»œè¿æ¥ 2) å°è¯• 'wandb login --relogin' 3) æ£€æŸ¥é¡¹ç›®æƒé™")
                        args.use_wandb = False
                        return False

        return False
    return False


def train(args):
    """å¢å¼ºç‰ˆè®­ç»ƒå‡½æ•°"""
    print("======================å¼€å§‹æ°´èµ„æºç®¡ç†è®­ç»ƒ==========================")

    print("è®­ç»ƒå‚æ•°æ£€æŸ¥:")
    print(f"   - use_wandb: {getattr(args, 'use_wandb', 'NOT_SET')}")
    print(f"   - wandb_project: {getattr(args, 'wandb_project', 'NOT_SET')}")
    print(f"   - run_dir: {getattr(args, 'run_dir', 'NOT_SET')}")
    print(f"   - num_env_steps: {getattr(args, 'num_env_steps', 'NOT_SET')}")
    print(f"   - episode_length: {getattr(args, 'episode_length', 'NOT_SET')}")
    print(f"   - ENHANCED_LOGGER_AVAILABLE: {ENHANCED_LOGGER_AVAILABLE}")

    if not hasattr(args, 'use_wandb'):
        args.use_wandb = True
        print("å¯ç”¨use_wandb")

    if not hasattr(args, 'wandb_project'):
        args.wandb_project = 'uncategorized'

    if not hasattr(args, 'enable_optimizations'):
        args.enable_optimizations = False

    if not hasattr(args, 'log_interval'):
        args.log_interval = 10

    if not hasattr(args, 'device'):
        args.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # åˆå§‹åŒ–WandBè®°å½•å™¨
    enhanced_logger = None

    if args.use_wandb and ENHANCED_LOGGER_AVAILABLE:
        try:
            enhanced_logger = EnhancedWandBLogger(log_dir=f"{args.run_dir}/wandb_data")
            print(f" WandBè®°å½•å™¨åˆå§‹åŒ–æˆåŠŸ: {args.run_dir}/wandb_data")
        except Exception as e:
            print(f" WandBè®°å½•å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            enhanced_logger = None
    else:
        print(f" ä¸æ»¡è¶³åˆå§‹åŒ–æ¡ä»¶: use_wandb={args.use_wandb}, ENHANCED_LOGGER_AVAILABLE={ENHANCED_LOGGER_AVAILABLE}")

    print(f" Enhanced logger æœ€ç»ˆçŠ¶æ€: {enhanced_logger is not None}")
    if enhanced_logger:
        print(f" Enhanced logger ä¿å­˜ç›®å½•: {enhanced_logger.log_dir}")

    # åˆå§‹åŒ–WandB
    if args.use_wandb and not hasattr(wandb, 'run') or wandb.run is None:
        use_wandb = initialize_wandb(args)
        print(f" WandBåˆå§‹åŒ–ç»“æœ: {use_wandb}")
    else:
        use_wandb = args.use_wandb
        if use_wandb:
            print(f"ä½¿ç”¨ç°æœ‰çš„ WandB ä¼šè¯: {wandb.run.name if wandb.run else 'Unknown'}")

    device = args.device
    print(f" ä½¿ç”¨è®¾å¤‡: {device}")

    #  åˆ›å»ºç¯å¢ƒ
    print(" å¼€å§‹åˆ›å»ºç¯å¢ƒ...")
    env = make_train_env(args)
    eval_env = make_train_env(args)
    print(" ç¯å¢ƒåˆ›å»ºå®Œæˆ")

    # åˆå§‹åŒ–ç­–ç•¥
    print(" å¼€å§‹åˆå§‹åŒ–ç­–ç•¥...")
    policy, trainer = init_hierarchical_policy(env, args)
    print(" ç­–ç•¥åˆå§‹åŒ–å®Œæˆ")

    n_rollout_threads = 1

    # è®¡ç®—åŠ¨ä½œç©ºé—´
    act_space_list = convert_action_space_for_mappo(env)
    first_agent = list(env.agents)[0]
    unified_obs_space = env.observation_space[first_agent]
    unified_action_space = act_space_list[0]

    buffer = SharedReplayBuffer(
        args, args.num_agents, unified_obs_space,
        unified_obs_space, unified_action_space
    )
    print(" [DEBUG] Bufferåˆ›å»ºå®Œæˆ")

    # åˆå§‹åŒ–ç›‘æ§å™¨
    monitor = EnhancedTrainingMonitor(
        log_interval=args.log_interval,
    )

    print(" [DEBUG] ç›‘æ§å™¨åˆ›å»ºå®Œæˆ")

    # ç”¨äºè¿½è¸ªæœ€ä½³æ€§èƒ½çš„å˜é‡
    best_eval_reward = -np.inf
    no_improvement_count = 0
    max_no_improvement = 50  # 50æ¬¡è¯„ä¼°æ— æ”¹å–„åˆ™æ—©åœ

    # ğŸ”§ ç”¨äºè¿½è¸ªæ”¶æ•›çš„å˜é‡
    episode_rewards = deque(maxlen=100)  # ä¿å­˜æœ€è¿‘100ä¸ªepisodeçš„å¥–åŠ±
    total_steps = 0
    episode = 0

    if getattr(args, 'enable_optimizations', False):
        print(" ä¼˜åŒ–å·²å¯ç”¨:")
    else:
        print(" ä½¿ç”¨æ ‡å‡†è®­ç»ƒé…ç½®")

    #  æå‰è®¡ç®—æœ€å¤§æ­¥æ•°
    max_episodes = args.num_env_steps // args.episode_length
    print(f" ç›®æ ‡: {max_episodes} episodes, æ¯ä¸ªepisode {args.episode_length} æ­¥")

    #  è®­ç»ƒçŠ¶æ€æ ‡è®°
    training_completed = False

    torch.cuda.empty_cache() if torch.cuda.is_available() else None

    print(f" å¼€å§‹è®­ç»ƒå¾ªç¯ï¼Œæœ€å¤§episodes: {getattr(args, 'max_episodes', 1000)}")
    print(f" [DEBUG] è®­ç»ƒå¾ªç¯å¼€å§‹å‰çŠ¶æ€:")
    print(f"   - enhanced_logger: {enhanced_logger is not None}")
    print(f"   - use_wandb: {use_wandb}")
    print(f"   - training_completed: {training_completed}")
    print(f"   - max_episodes: {max_episodes}")

    # è®­ç»ƒä¸»å¾ªç¯
    try:
        while not training_completed:
            if total_steps >= args.num_env_steps:
                print(f" è¾¾åˆ°æœ€å¤§è®­ç»ƒæ­¥æ•° {args.num_env_steps:,}")
                break

            if episode >= getattr(args, 'max_episodes', 1000):
                print(f" è¾¾åˆ°æœ€å¤§è®­ç»ƒå›åˆæ•° {getattr(args, 'max_episodes', 1000)}")
                break

            # Episodeå¼€å§‹
            episode_start_time = time.time()

            # ğŸ”§ [DEBUG] æ¯10ä¸ªepisodeæ‰“å°ä¸€æ¬¡çŠ¶æ€
            if episode % 10 == 0:
                print(f" [DEBUG] Episode {episode + 1} å¼€å§‹:")
                print(f"   - enhanced_loggerå­˜åœ¨: {enhanced_logger is not None}")
                print(f"   - total_steps: {total_steps}")
                print(f"   - å†å²æ•°æ®é•¿åº¦: {len(enhanced_logger.episode_history) if enhanced_logger else 0}")

            try:
                # é‡ç½®ç¯å¢ƒ
                obs = env.reset()

                rnn_states_actor = np.zeros((args.num_agents, args.recurrent_N, args.hidden_size), dtype=np.float32)
                rnn_states_critic = np.zeros_like(rnn_states_actor)
                masks = np.ones((args.num_agents, 1), dtype=np.float32)

                episode_reward = 0
                episode_steps = 0
                episode_done = False

                # Episodeå¾ªç¯
                while not episode_done and episode_steps < args.episode_length:
                    if total_steps >= args.num_env_steps:
                        training_completed = True
                        break

                    try:
                        # ç¯å¢ƒäº¤äº’
                        obs_list = [obs[agent] for agent in env.agents]

                        with torch.no_grad():
                            obs_array = np.array(obs_list, dtype=np.float32)
                            obs_tensor = torch.from_numpy(obs_array).unsqueeze(0).to(args.device)
                            share_obs_tensor = obs_tensor

                            rnn_states_actor_tensor = torch.from_numpy(rnn_states_actor).unsqueeze(0).to(args.device)
                            rnn_states_critic_tensor = torch.from_numpy(rnn_states_critic).unsqueeze(0).to(args.device)
                            masks_tensor = torch.from_numpy(masks).unsqueeze(0).to(args.device)

                            # è·å–åŠ¨ä½œ
                            value, action, action_log_prob, rnn_states_actor_new, rnn_states_critic_new = policy.get_actions(
                                share_obs_tensor,
                                obs_tensor,
                                rnn_states_actor_tensor,
                                rnn_states_critic_tensor,
                                masks_tensor
                            )

                        rnn_states_actor = safe_tensor_to_numpy(rnn_states_actor_new.squeeze(0))
                        rnn_states_critic = safe_tensor_to_numpy(rnn_states_critic_new.squeeze(0))

                        actions_np = safe_tensor_to_numpy(action.squeeze(0))
                        actions_dict = convert_actions_from_mappo(env, actions_np)

                        # ç¯å¢ƒæ­¥è¿›
                        next_obs, rewards, dones, truncations, infos = env.step(actions_dict)

                        # æ›´æ–°masks
                        masks_old = masks.copy()
                        masks = np.ones((args.num_agents, 1), dtype=np.float32)
                        for i, agent_id in enumerate(env.agents):
                            if dones[agent_id] or any(t for t in truncations.values() if t):
                                masks[i] = 0.0

                        rewards_list = [rewards[agent] for agent in env.agents]
                        rewards_np = np.array(rewards_list, dtype=np.float32).reshape(1, -1, 1)

                        # è½¬æ¢numpyçŠ¶æ€ä¸ºtensorç”¨äºbufferå­˜å‚¨
                        rnn_states_actor_for_buffer = torch.from_numpy(rnn_states_actor).to(args.device)
                        rnn_states_critic_for_buffer = torch.from_numpy(rnn_states_critic).to(args.device)
                        masks_old_tensor = torch.from_numpy(masks_old).to(args.device)

                        buffer.insert(
                            share_obs_tensor,
                            obs_tensor,
                            rnn_states_actor_for_buffer,
                            rnn_states_critic_for_buffer,
                            action,
                            action_log_prob,
                            value,
                            torch.from_numpy(rewards_np).to(args.device),
                            masks_old_tensor
                        )

                        # æ›´æ–°çŠ¶æ€
                        obs = next_obs
                        step_reward = np.mean(list(rewards.values()))
                        episode_reward += step_reward / args.episode_length
                        total_steps += 1
                        episode_steps += 1

                        # ğŸ†• æ·»åŠ stepçº§åˆ«çš„è¯¦ç»†wandbè®°å½•
                        if use_wandb and total_steps % 100 == 0:  # æ¯100æ­¥è®°å½•ä¸€æ¬¡
                            try:
                                step_log_data = {
                                    "step/total_steps": int(total_steps),
                                    "step/episode": int(episode + 1),
                                    "step/episode_step": int(episode_steps),
                                    "step/reward": float(step_reward),
                                    "step/episode_reward_cumulative": float(episode_reward),
                                    "step/timestamp": time.time()
                                }

                                # è®°å½•å¹³å‡å¥–åŠ±
                                if rewards:
                                    agent_rewards = list(rewards.values())
                                    step_log_data["step/agent_rewards_mean"] = float(np.mean(agent_rewards))
                                    step_log_data["step/agent_rewards_std"] = float(np.std(agent_rewards))
                                    step_log_data["step/agent_rewards_max"] = float(np.max(agent_rewards))
                                    step_log_data["step/agent_rewards_min"] = float(np.min(agent_rewards))

                                if 'value' in locals():
                                    step_log_data["step/value_function"] = float(value.mean().item())

                                if 'actions_np' in locals():
                                    step_log_data["step/action_mean"] = float(np.mean(actions_np))
                                    step_log_data["step/action_std"] = float(np.std(actions_np))

                                if total_steps % 500 == 0:  # æ¯500æ­¥è®°å½•ä¸€æ¬¡ç³»ç»ŸæŒ‡æ ‡
                                    step_log_data["system/memory_usage_mb"] = float(psutil.virtual_memory().used / 1024 / 1024)
                                    step_log_data["system/memory_percent"] = float(psutil.virtual_memory().percent)
                                    step_log_data["system/cpu_percent"] = float(psutil.cpu_percent())

                                    # æ·»åŠ CUDAå†…å­˜ç›‘æ§
                                    if torch.cuda.is_available():
                                        step_log_data["system/gpu_memory_used_mb"] = float(torch.cuda.memory_allocated() / 1024 / 1024)
                                        step_log_data["system/gpu_memory_cached_mb"] = float(torch.cuda.memory_reserved() / 1024 / 1024)

                                # è®°å½•ç¯å¢ƒçŠ¶æ€ä¿¡æ¯
                                if infos:
                                    first_agent_info = list(infos.values())[0]
                                    if isinstance(first_agent_info, dict):
                                        if 'supply_satisfaction_rate' in first_agent_info:
                                            step_log_data["step/supply_satisfaction"] = float(first_agent_info['supply_satisfaction_rate'])
                                        if 'avg_reservoir_level' in first_agent_info:
                                            step_log_data["step/reservoir_level"] = float(first_agent_info['avg_reservoir_level'])
                                        if 'ecological_flow_deviation' in first_agent_info:
                                            step_log_data["step/ecological_deviation"] = float(first_agent_info['ecological_flow_deviation'])

                                        # å¥–åŠ±è®°å½•
                                        if 'reward_components' in first_agent_info:
                                            reward_components = first_agent_info['reward_components']
                                            if isinstance(reward_components, dict):

                                                total_reward = sum(v for v in reward_components.values() if isinstance(v, (int, float)) and not (isinstance(v, float) and (math.isnan(v) or math.isinf(v))))
                                                step_log_data["step/reward_components_total"] = float(total_reward)

                                        if 'phase' in first_agent_info:
                                            step_log_data["step/phase"] = str(first_agent_info['phase'])
                                        if 'exploration_active' in first_agent_info:
                                            step_log_data["step/exploration_active"] = int(bool(first_agent_info['exploration_active']))

                                # æ°´åº“æŒ‡æ ‡
                                if hasattr(env, 'reservoirs') and hasattr(env, 'max_reservoir'):
                                    total_volume = np.sum(env.reservoirs)
                                    total_capacity = np.sum(env.max_reservoir)
                                    reservoir_levels = env.reservoirs / env.max_reservoir

                                    step_log_data["reservoirs/total_volume"] = float(total_volume)
                                    step_log_data["reservoirs/total_capacity"] = float(total_capacity)
                                    step_log_data["reservoirs/avg_level"] = float(np.mean(reservoir_levels))
                                    step_log_data["reservoirs/min_level"] = float(np.min(reservoir_levels))
                                    step_log_data["reservoirs/max_level"] = float(np.max(reservoir_levels))
                                    step_log_data["reservoirs/level_std"] = float(np.std(reservoir_levels))

                                # æ°´å‚æŒ‡æ ‡
                                if hasattr(env, 'plants_demand'):
                                    total_demand = np.sum(env.plants_demand)
                                    step_log_data["plants/total_demand"] = float(total_demand)
                                    step_log_data["plants/avg_demand"] = float(np.mean(env.plants_demand))

                                # ä¾›æ°´æŒ‡æ ‡
                                if hasattr(env, '_last_actual_supply'):
                                    actual_supply = env._last_actual_supply
                                    total_supply = np.sum(actual_supply)
                                    step_log_data["supply/total_supply"] = float(total_supply)
                                    step_log_data["supply/avg_supply"] = float(np.mean(actual_supply))

                                    # ä¾›éœ€å¹³è¡¡
                                    if hasattr(env, 'plants_demand'):
                                        total_demand = np.sum(env.plants_demand) / 24.0  # å°æ—¶éœ€æ±‚
                                        if total_demand > 0:
                                            step_log_data["balance/supply_demand_ratio"] = float(total_supply / total_demand)
                                            step_log_data["balance/demand_satisfaction_rate"] = float(min(1.0, total_supply / total_demand))

                                # å®‰å…¨æŒ‡æ ‡
                                if hasattr(env, 'reservoirs') and hasattr(env, 'max_reservoir'):
                                    reservoir_levels = env.reservoirs / env.max_reservoir
                                    low_level_count = np.sum(reservoir_levels < 0.4)
                                    critical_count = np.sum(reservoir_levels < 0.2)
                                    high_level_count = np.sum(reservoir_levels > 0.8)

                                    step_log_data["safety/low_level_reservoirs"] = int(low_level_count)
                                    step_log_data["safety/critical_level_reservoirs"] = int(critical_count)
                                    step_log_data["safety/high_level_reservoirs"] = int(high_level_count)
                                    step_log_data["safety/safe_reservoirs"] = int(len(env.reservoirs) - low_level_count - high_level_count - critical_count)

                                wandb.log(step_log_data, step=total_steps)

                            except Exception as e:
                                print(f" Stepçº§åˆ«wandbè®°å½•å¤±è´¥: {e}")

                        if use_wandb and total_steps % 1000 == 0:
                            try:
                                detailed_snapshot = {
                                    "detailed/total_steps": int(total_steps),
                                    "detailed/episode": int(episode + 1),
                                    "detailed/timestamp": time.time()
                                }

                                # è¯¦ç»†çš„æ°´åº“çŠ¶æ€ï¼ˆæ¯1000æ­¥è®°å½•ä¸€æ¬¡ï¼‰
                                if hasattr(env, 'reservoirs') and hasattr(env, 'max_reservoir'):
                                    for i in range(min(len(env.reservoirs), 5)):  # åªè®°å½•å‰5ä¸ªæ°´åº“
                                        current_level = float(env.reservoirs[i]) / float(env.max_reservoir[i]) if env.max_reservoir[i] > 0 else 0.0
                                        detailed_snapshot[f"detailed/reservoir_{i}_level"] = current_level

                                # è¯¦ç»†çš„æ™ºèƒ½ä½“å¥–åŠ±ï¼ˆæŠ½æ ·è®°å½•ï¼‰
                                if rewards:
                                    sample_agents = list(rewards.keys())[:5]  # å‰5ä¸ªæ™ºèƒ½ä½“
                                    for agent_id in sample_agents:
                                        detailed_snapshot[f"detailed/agent_rewards/{agent_id}"] = float(rewards[agent_id])

                                wandb.log(detailed_snapshot, step=total_steps)
                                print(f" è¯¦ç»†å¿«ç…§å·²è®°å½•: Step {total_steps}")

                            except Exception as e:
                                print(f" è¯¦ç»†å¿«ç…§è®°å½•å¤±è´¥: {e}")

                        # æ£€æŸ¥episodeç»“æŸ
                        if all(dones.values()) or any(truncations.values()):
                            episode_done = True

                            # è®¡ç®—ä¼˜åŠ¿å’Œæ›´æ–°ç­–ç•¥
                            try:
                                with torch.no_grad():
                                    next_obs_array = np.array([next_obs[agent] for agent in env.agents],
                                                              dtype=np.float32)
                                    next_obs_tensor = torch.from_numpy(next_obs_array).unsqueeze(0).to(args.device)

                                    # ğŸ”§ ä¿®å¤7: æ­£ç¡®å¤„ç†æœ€ç»ˆRNNçŠ¶æ€
                                    final_rnn_states_critic = torch.from_numpy(rnn_states_critic).unsqueeze(0).to(
                                        args.device)
                                    final_masks = torch.from_numpy(masks).unsqueeze(0).to(args.device)

                                    next_value = policy.get_values(
                                        next_obs_tensor,
                                        final_rnn_states_critic,
                                        final_masks
                                    )

                                buffer.compute_returns(next_value, policy.critic)
                                train_info = trainer.train(buffer)
                                buffer.after_update()

                                # æ›´æ–°çš„è¯¦ç»†wandbè®°å½•
                                if use_wandb and train_info:
                                    try:
                                        policy_log_data = {
                                            "policy/episode": int(episode + 1),
                                            "policy/total_steps": int(total_steps),
                                            "policy/update_timestamp": time.time()
                                        }

                                        # è®­ç»ƒæŒ‡æ ‡
                                        if 'value_loss' in train_info:
                                            policy_log_data["policy/value_loss"] = float(train_info['value_loss'])
                                        if 'policy_loss' in train_info:
                                            policy_log_data["policy/policy_loss"] = float(train_info['policy_loss'])
                                        if 'entropy' in train_info:
                                            policy_log_data["policy/entropy"] = float(train_info['entropy'])
                                        if 'ratio' in train_info:
                                            policy_log_data["policy/ratio"] = float(train_info['ratio'])
                                        if 'surr1' in train_info:
                                            policy_log_data["policy/surr1"] = float(train_info['surr1'])
                                        if 'surr2' in train_info:
                                            policy_log_data["policy/surr2"] = float(train_info['surr2'])
                                        if 'kl_div' in train_info:
                                            policy_log_data["policy/kl_divergence"] = float(train_info['kl_div'])
                                        if 'clip_frac' in train_info:
                                            policy_log_data["policy/clip_fraction"] = float(train_info['clip_frac'])
                                        if 'grad_norm' in train_info:
                                            policy_log_data["policy/grad_norm"] = float(train_info['grad_norm'])

                                        # å­¦ä¹ ç‡
                                        if hasattr(trainer, 'lr_scheduler') and trainer.lr_scheduler:
                                            policy_log_data["policy/learning_rate"] = float(trainer.lr_scheduler.get_last_lr()[0])
                                        elif hasattr(trainer, 'optimizer') and trainer.optimizer:
                                            policy_log_data["policy/learning_rate"] = float(trainer.optimizer.param_groups[0]['lr'])

                                        # ç½‘ç»œå‚æ•°ç»Ÿè®¡
                                        if hasattr(policy, 'actor') and policy.actor:
                                            actor_params = []
                                            for param in policy.actor.parameters():
                                                if param.requires_grad:
                                                    actor_params.append(param.data.cpu().numpy().flatten())
                                            if actor_params:
                                                actor_params = np.concatenate(actor_params)
                                                policy_log_data["policy/actor_param_mean"] = float(np.mean(actor_params))
                                                policy_log_data["policy/actor_param_std"] = float(np.std(actor_params))

                                        if hasattr(policy, 'critic') and policy.critic:
                                            critic_params = []
                                            for param in policy.critic.parameters():
                                                if param.requires_grad:
                                                    critic_params.append(param.data.cpu().numpy().flatten())
                                            if critic_params:
                                                critic_params = np.concatenate(critic_params)
                                                policy_log_data["policy/critic_param_mean"] = float(np.mean(critic_params))
                                                policy_log_data["policy/critic_param_std"] = float(np.std(critic_params))

                                        wandb.log(policy_log_data, step=total_steps)
                                        print(f" ç­–ç•¥æ›´æ–°è®°å½•: Episode {episode + 1}, æŒ‡æ ‡æ•°: {len(policy_log_data)}")

                                    except Exception as e:
                                        print(f" ç­–ç•¥æ›´æ–°wandbè®°å½•å¤±è´¥: {e}")

                            except Exception as e:
                                print(f" ç­–ç•¥æ›´æ–°å‡ºé”™: {e}")
                                import traceback
                                traceback.print_exc()
                                episode_done = True

                            break

                    except Exception as e:
                        print(f" ç¯å¢ƒäº¤äº’å‡ºé”™: {e}")
                        import traceback
                        traceback.print_exc()

                        infos = {agent: {} for agent in env.agents}
                        episode_done = True
                        break

                if training_completed:
                    break

                episode_time = time.time() - episode_start_time
                if 'infos' not in locals():
                    infos = {agent: {} for agent in env.agents}

                # æ ‡å‡†ç›‘æ§å™¨è®°å½•
                monitor.log_episode(episode + 1, episode_reward, episode_steps, total_steps, infos)

                # å¢å¼ºWandBè®°å½•å™¨è®°å½•ï¼ˆç¡®ä¿å®Œæ•´å†å²æ•°æ®ä¿å­˜ï¼‰
                if enhanced_logger:
                    try:
                        print(f" [DEBUG] Episode {episode + 1} å°è¯•è®°å½•åˆ°å¢å¼ºè®°å½•å™¨...")
                        enhanced_logger.log_episode_complete(
                            episode + 1, episode_reward, episode_steps, total_steps, infos
                        )
                        print(f" [DEBUG] å¢å¼ºè®°å½•å™¨å·²ä¿å­˜Episode {episode + 1}å®Œæ•´æ•°æ®ï¼Œå½“å‰å†å²é•¿åº¦: {len(enhanced_logger.episode_history)}")
                    except Exception as e:
                        print(f"ï¸ å¢å¼ºè®°å½•å™¨è®°å½•å¤±è´¥: {e}")
                else:
                    print(f"ï¸ [DEBUG] Episode {episode + 1} è·³è¿‡å¢å¼ºè®°å½•å™¨è®°å½•ï¼ˆenhanced_loggerä¸ºNoneï¼‰")

            except Exception as e:
                print(f" Episode {episode + 1} å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()

                if 'infos' not in locals():
                    infos = {agent: {} for agent in env.agents}

                episode_time = time.time() - episode_start_time
                monitor.log_episode(episode + 1, 0.0, episode_steps, total_steps, infos)

                if enhanced_logger:
                    try:
                        enhanced_logger.log_episode_complete(
                            episode + 1, 0.0, episode_steps, total_steps, infos
                        )
                    except Exception as e2:
                        print(f" å¼‚å¸¸æƒ…å†µå¢å¼ºè®°å½•å™¨å¤±è´¥: {e2}")

                episode += 1
                continue

            # è®°å½•å¢å¼ºåˆ†æ
            if (episode + 1) % 5 == 0:  # æ¯5ä¸ªepisodeè®°å½•ä¸€æ¬¡å¢å¼ºåˆ†æ
                monitor.log_enhanced_analysis(env)

            # W&Bè®°å½•
            if use_wandb:
                try:

                    step_reward = np.mean(list(rewards.values()))

                    # åŸºç¡€å¥–åŠ±è®°å½•
                    log_data = {
                        "train/episode": int(episode + 1),
                        "train/episode_steps": int(episode_steps),
                        "train/total_steps": int(total_steps),
                        "train/episode_time": float(episode_time),
                        "global_step": int(total_steps)
                    }

                    # è®°å½•ä¸åŒå°ºåº¦çš„å¥–åŠ±
                    # 1. æ­¥å¥–åŠ± (å•æ­¥å¥–åŠ±)
                    log_data["rewards/step_reward"] = float(step_reward)

                    # 2. ç´¯ç§¯å¥–åŠ± (Episodeæ€»å¥–åŠ±)
                    log_data["rewards/episode_reward_total"] = float(episode_reward)

                    # 3. æ ‡å‡†åŒ–å¥–åŠ±
                    normalized_episode_reward = episode_reward / episode_steps if episode_steps > 0 else 0
                    log_data["rewards/episode_reward_normalized"] = float(normalized_episode_reward)

                    # 4. æ—¥å‡å¥–åŠ±
                    daily_reward = episode_reward / (episode_steps / 24.0) if episode_steps > 0 else 0
                    log_data["rewards/daily_reward"] = float(daily_reward)

                    # 5. å°æ—¶å¥–åŠ±
                    hourly_reward = episode_reward / episode_steps if episode_steps > 0 else 0
                    log_data["rewards/hourly_reward"] = float(hourly_reward)


                    if 'infos' in locals() and infos:

                        first_agent_info = None
                        for agent_id, info in infos.items():
                            if isinstance(info, dict):
                                first_agent_info = info
                                break

                        if first_agent_info:
                            # æ€§èƒ½æŒ‡æ ‡
                            if 'supply_satisfaction_rate' in first_agent_info:
                                log_data["metrics/supply_satisfaction"] = float(
                                    first_agent_info['supply_satisfaction_rate'])

                            if 'avg_reservoir_level' in first_agent_info:
                                log_data["metrics/reservoir_safety"] = float(
                                    first_agent_info['avg_reservoir_level'])

                            # å¥–åŠ±ç»„ä»¶åˆ†è§£
                            if 'reward_components' in first_agent_info:
                                reward_components = first_agent_info['reward_components']
                                if isinstance(reward_components, dict):
                                    # åŸå§‹å¥–åŠ±ç»„ä»¶
                                    for component, value in reward_components.items():
                                        if isinstance(value, (int, float)) and not (isinstance(value, float) and (
                                                math.isnan(value) or math.isinf(value))):
                                            log_data[f"rewards/components/{component}"] = float(value)

                                    total_supply_reward = reward_components.get('supply', 0) * episode_steps
                                    total_safety_reward = reward_components.get('safety', 0) * episode_steps
                                    total_ecological_reward = reward_components.get('ecological', 0) * episode_steps

                                    log_data["rewards/cumulative/supply"] = float(total_supply_reward)
                                    log_data["rewards/cumulative/safety"] = float(total_safety_reward)
                                    log_data["rewards/cumulative/ecological"] = float(total_ecological_reward)

                                    # ğŸš¨ æ–°å¢ï¼šå¥–åŠ±ç»„ä»¶çš„æƒé‡åˆ†å¸ƒ
                                    total_component_reward = sum(abs(v) for v in reward_components.values())
                                    if total_component_reward > 0:
                                        for component, value in reward_components.items():
                                            weight = abs(value) / total_component_reward
                                            log_data[f"rewards/weights/{component}"] = float(weight)

                            # ğŸš¨ æ–°å¢ï¼šæ™ºèƒ½ä½“çº§åˆ«çš„å¥–åŠ±åˆ†è§£
                            agent_rewards = {}
                            for agent_id, info in infos.items():
                                if isinstance(info, dict) and 'total_reward' in info:
                                    agent_rewards[agent_id] = info['total_reward']

                            if agent_rewards:
                                # æ™ºèƒ½ä½“å¥–åŠ±ç»Ÿè®¡
                                agent_reward_values = list(agent_rewards.values())
                                log_data["rewards/agents/mean"] = float(np.mean(agent_reward_values))
                                log_data["rewards/agents/std"] = float(np.std(agent_reward_values))
                                log_data["rewards/agents/max"] = float(np.max(agent_reward_values))
                                log_data["rewards/agents/min"] = float(np.min(agent_reward_values))

                                # ä¸ªä½“æ™ºèƒ½ä½“å¥–åŠ±
                                for agent_id, reward in agent_rewards.items():
                                    log_data[f"rewards/individual/{agent_id}"] = float(reward)

                            # è®­ç»ƒé˜¶æ®µ
                            if 'phase' in first_agent_info:
                                log_data["training/phase"] = str(first_agent_info['phase'])

                            # æ¢ç´¢çŠ¶æ€
                            if 'exploration_active' in first_agent_info:
                                log_data["training/exploration_active"] = int(
                                    bool(first_agent_info['exploration_active']))

                            if 'diversity_score' in first_agent_info:
                                diversity_val = first_agent_info['diversity_score']
                                if isinstance(diversity_val, (int, float)) and not (
                                        isinstance(diversity_val, float) and (
                                        math.isnan(diversity_val) or math.isinf(diversity_val))):
                                    log_data["training/diversity_score"] = float(diversity_val)

                        # å¥–åŠ±è¶‹åŠ¿åˆ†æ
                        if hasattr(monitor, 'episode_rewards') and len(monitor.episode_rewards) > 10:
                            recent_rewards = list(monitor.episode_rewards)[-10:]
                            log_data["rewards/trends/mean_10"] = float(np.mean(recent_rewards))
                            log_data["rewards/trends/std_10"] = float(np.std(recent_rewards))

                            if len(recent_rewards) >= 5:
                                first_half = np.mean(recent_rewards[:5])
                                second_half = np.mean(recent_rewards[5:])
                                trend = (second_half - first_half) / (abs(first_half) + 1e-8)
                                log_data["rewards/trends/direction"] = float(trend)

                        # è®°å½•å¹¶éªŒè¯
                        wandb.log(log_data)
                        print(f" wandb è®°å½•æˆåŠŸ: Episode {episode + 1}, æ•°æ®ç‚¹: {len(log_data)}")

                        # æ¯10ä¸ªepisodeè®°å½•èšåˆæ•°æ®
                        if (episode + 1) % 10 == 0:
                            aggregated_data = {
                                "milestones/episodes_10": episode + 1,
                                "milestones/total_steps_10": total_steps,
                                "milestones/timestamp": time.time()
                            }

                            # 10ä¸ªepisodeçš„èšåˆå¥–åŠ±ç»Ÿè®¡
                            if hasattr(monitor, 'episode_rewards') and len(monitor.episode_rewards) >= 10:
                                last_10_rewards = list(monitor.episode_rewards)[-10:]
                                aggregated_data.update({
                                    "milestones/reward_mean_10": float(np.mean(last_10_rewards)),
                                    "milestones/reward_std_10": float(np.std(last_10_rewards)),
                                    "milestones/reward_max_10": float(np.max(last_10_rewards)),
                                    "milestones/reward_min_10": float(np.min(last_10_rewards))
                                })

                            wandb.log(aggregated_data)

                except Exception as e:
                    print(f" W&Bè®°å½•è¯¦ç»†é”™è¯¯: {e}")
                    import traceback
                    print(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")

                    # ç¯å¢ƒæŒ‡æ ‡
                    if infos:
                        first_agent_info = list(infos.values())[0]
                        if isinstance(first_agent_info, dict):
                            if 'supply_satisfaction_rate' in first_agent_info:
                                log_data["metrics/supply_satisfaction"] = float(
                                    first_agent_info['supply_satisfaction_rate'])
                            if 'avg_reservoir_level' in first_agent_info:
                                log_data["metrics/reservoir_safety"] = float(
                                    first_agent_info['avg_reservoir_level'])
                            if 'reward_components' in first_agent_info:
                                reward_components = first_agent_info['reward_components']
                                for component, value in reward_components.items():
                                    if isinstance(value, (int, float)):
                                        log_data[f"rewards/{component}"] = float(value)

                    wandb.log(log_data)
                    print(f" å¼ºåˆ¶ wandb è®°å½•: Episode {episode + 1}, æ•°æ®ç‚¹: {len(log_data)}")

                except Exception as e:
                    print(f" W&Bè®°å½•è¯¦ç»†é”™è¯¯: {e}")
                    import traceback
                    print(traceback.format_exc())

            # å†…å­˜æ¸…ç†
            if (episode + 1) % 50 == 0:
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

            # æ–°å¢ï¼šå®šæœŸä¿å­˜å†å²æ•°æ®ï¼ˆæ¯100ä¸ªEpisodeï¼‰
            if enhanced_logger and (episode + 1) % 100 == 0:
                try:
                    print(f"ğŸ”§ [DEBUG] Episode {episode + 1} è§¦å‘å®šæœŸä¿å­˜...")
                    print(f"ğŸ”§ [DEBUG] å½“å‰å†å²æ•°æ®: Episodes={len(enhanced_logger.episode_history)}, Steps={len(enhanced_logger.step_history)}")
                    enhanced_logger.save_history_files()
                    print(f"ğŸ’¾ å®šæœŸä¿å­˜å†å²æ•°æ®: Episode {episode + 1}")

                    # åˆ›å»ºæ£€æŸ¥ç‚¹å…¼å®¹æ–‡ä»¶
                    checkpoint_file = enhanced_logger.create_wandb_compatible_history()
                    if checkpoint_file:
                        print(f"âœ… æ£€æŸ¥ç‚¹æ–‡ä»¶å·²æ›´æ–°: {checkpoint_file}")
                    else:
                        print(f"ï¸ [DEBUG] æ£€æŸ¥ç‚¹æ–‡ä»¶åˆ›å»ºå¤±è´¥")

                    #  ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
                    current_avg_reward = np.mean(list(monitor.episode_rewards)[-10:]) if len(monitor.episode_rewards) >= 10 else 0.0
                    checkpoint_path = Path(args.run_dir) / f"checkpoint_episode_{episode + 1}.pt"
                    torch.save({
                        'actor_state_dict': policy.actor.state_dict(),
                        'critic_state_dict': policy.critic.state_dict(),
                        'episode': episode + 1,
                        'total_steps': total_steps,
                        'avg_reward': current_avg_reward,
                        'timestamp': time.time()
                    }, str(checkpoint_path))
                    print(f" æ¨¡å‹æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path.name}")

                except Exception as e:
                    print(f" å®šæœŸä¿å­˜å¤±è´¥: {e}")
            elif enhanced_logger and (episode + 1) % 10 == 0:
                print(f" [DEBUG] Episode {episode + 1} æ£€æŸ¥ç‚¹ï¼šå†å²é•¿åº¦ {len(enhanced_logger.episode_history)}, è·ç¦»ä¿å­˜è¿˜æœ‰ {100 - ((episode + 1) % 100)} episodes")
            elif not enhanced_logger and (episode + 1) % 100 == 0:
                print(f"ï¸ [DEBUG] Episode {episode + 1} åº”è¯¥å®šæœŸä¿å­˜ï¼Œbut enhanced_logger is None")

            # è¯„ä¼°
            if (episode + 1) % args.eval_interval == 0:
                print(f"\n ç¬¬ {episode + 1} æ¬¡è¯„ä¼°")
                try:
                    eval_reward = evaluate_optimized(policy, eval_env, args.eval_episodes, args.episode_length,
                                                     args)
                    monitor.log_eval(eval_reward)

                    if use_wandb:
                        try:
                            wandb.log({
                                "eval/reward": eval_reward,
                                "global_step": total_steps
                            })
                        except Exception as e:
                            print(f" W&Bè¯„ä¼°è®°å½•å¤±è´¥: {e}")

                        # æ—©åœæ£€æŸ¥
                        if eval_reward > best_eval_reward:
                            best_eval_reward = eval_reward
                            no_improvement_count = 0

                            # ä¿å­˜æœ€ä½³æ¨¡å‹
                            try:
                                save_path = Path(args.run_dir) / "best_model.pt"
                                torch.save({
                                    'actor_state_dict': policy.actor.state_dict(),
                                    'critic_state_dict': policy.critic.state_dict(),
                                    'episode': episode,
                                    'best_eval_reward': best_eval_reward,
                                    'total_steps': total_steps
                                }, str(save_path))
                                print(f" ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œå¥–åŠ±: {best_eval_reward:.3f}")
                            except Exception as e:
                                print(f"ï¸ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
                        else:
                            no_improvement_count += 1
                            print(f" æ— æ”¹å–„: {no_improvement_count}/{max_no_improvement}")

                            if no_improvement_count >= max_no_improvement:
                                print(f" æ—©åœè§¦å‘")
                                training_completed = True
                                break

                except Exception as e:
                    print(f"ï¸ è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {e}")

            episode += 1

    except KeyboardInterrupt:
        print("\nï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        training_completed = True

    finally:
        # è®­ç»ƒç»“æŸæ¸…ç†
        final_stats = monitor.get_stats()
        print(f"\n è®­ç»ƒç»“æŸ")
        print(f" æœ€ç»ˆç»Ÿè®¡: {final_stats}")

        #  ä¿å­˜å¢å¼ºè®°å½•å™¨çš„å®Œæ•´å†å²æ•°æ®
        if enhanced_logger:
            try:
                # ä¿å­˜å†å²æ–‡ä»¶å¹¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                enhanced_logger.save_history_files(clean_temp_files=True)

                # åˆ›å»ºwandbå…¼å®¹çš„å†å²æ–‡ä»¶
                wandb_history_file = enhanced_logger.create_wandb_compatible_history()
                if wandb_history_file:
                    print(f" WandBå…¼å®¹å†å²æ–‡ä»¶å·²åˆ›å»º: {wandb_history_file}")

                # è·å–è®­ç»ƒæ‘˜è¦
                summary = enhanced_logger.get_training_summary()
                print(f" è®­ç»ƒæ‘˜è¦:")
                print(f"   æ€»Episodes: {summary.get('total_episodes', 0):,}")
                print(f"   æ€»æ­¥æ•°: {summary.get('total_steps', 0):,}")
                print(f"   æœ€ç»ˆå¥–åŠ±: {summary.get('final_reward', 0):.3f}")
                print(f"   æœ€å¤§å¥–åŠ±: {summary.get('max_reward', 0):.3f}")
                print(f"   å¹³å‡å¥–åŠ±: {summary.get('avg_reward', 0):.3f}")
                print(f"   è®­ç»ƒæ—¶é•¿: {summary.get('training_duration', 0)/3600:.1f} å°æ—¶")

                # æ‘˜è¦è®°å½•åˆ°wandb
                if use_wandb:
                    try:
                        wandb.log({
                            "final/total_episodes": summary.get('total_episodes', 0),
                            "final/total_steps": summary.get('total_steps', 0),
                            "final/final_reward": summary.get('final_reward', 0),
                            "final/max_reward": summary.get('max_reward', 0),
                            "final/avg_reward": summary.get('avg_reward', 0),
                            "final/training_duration_hours": summary.get('training_duration', 0)/3600
                        })
                        print(" è®­ç»ƒæ‘˜è¦å·²è®°å½•åˆ°wandb")
                    except Exception as e:
                        print(f" è®­ç»ƒæ‘˜è¦wandbè®°å½•å¤±è´¥: {e}")

            except Exception as e:
                print(f" å¢å¼ºè®°å½•å™¨æ•°æ®ä¿å­˜å¤±è´¥: {e}")

        # æ¸…ç†èµ„æº
        monitor.cleanup()

        if use_wandb:
            try:
                wandb.log({"training_completed": True, "final_stats": final_stats})
            except Exception as e:
                print(f"ï¸ æœ€ç»ˆW&Bè®°å½•å¤±è´¥: {e}")


def evaluate_optimized(policy, env, num_episodes, episode_length, args):
    """è¯„ä¼°å‡½æ•°"""
    eval_rewards = []

    for eval_ep in range(num_episodes):
        try:
            obs = env.reset()
            episode_reward = 0
            episode_steps = 0

            rnn_states_actor = np.zeros((args.num_agents, args.recurrent_N, args.hidden_size), dtype=np.float32)
            masks = np.ones((args.num_agents, 1), dtype=np.float32)

            while episode_steps < episode_length:
                try:
                    with torch.no_grad():
                        obs_array = np.array([obs[agent] for agent in env.agents], dtype=np.float32)
                        obs_tensor = torch.from_numpy(obs_array).unsqueeze(0).to(args.device)
                        rnn_states_actor_tensor = torch.from_numpy(rnn_states_actor).unsqueeze(0).to(args.device)
                        masks_tensor = torch.from_numpy(masks).unsqueeze(0).to(args.device)

                        action, rnn_states_actor_new = policy.act(
                            obs_tensor,
                            rnn_states_actor_tensor,
                            masks_tensor,
                            deterministic=True
                        )

                        rnn_states_actor = safe_tensor_to_numpy(rnn_states_actor_new.squeeze(0))
                        actions_np = safe_tensor_to_numpy(action.squeeze(0))
                        actions = convert_actions_from_mappo(env, actions_np)

                    next_obs, rewards, dones, truncations, infos = env.step(actions)

                    episode_reward += np.mean(list(rewards.values()))
                    episode_steps += 1

                    # æ›´æ–°masks
                    masks = np.ones((args.num_agents, 1), dtype=np.float32)
                    for i, agent_id in enumerate(env.agents):
                        if dones[agent_id] or any(t for t in truncations.values() if t):
                            masks[i] = 0.0

                    if all(dones.values()) or any(truncations.values()):
                        break

                    obs = next_obs

                except Exception as e:
                    print(f"ï¸ è¯„ä¼°æ­¥éª¤å‡ºé”™: {e}")
                    break

            eval_rewards.append(episode_reward)

        except Exception as e:
            print(f"ï¸ è¯„ä¼°episode {eval_ep + 1} å‡ºé”™: {e}")
            continue

    return np.mean(eval_rewards) if eval_rewards else 0.0


def get_default_args():
    """è·å–é»˜è®¤å‚æ•°é…ç½®"""
    parser = get_config()

    return parser


def main():
    """å…¼å®¹wandb agentå’Œç›´æ¥è¿è¡Œ"""
    print(" å¼€å§‹æ°´èµ„æºç®¡ç†è®­ç»ƒ")
    print("=" * 60)

    try:
        # è·å–å‚æ•°é…ç½®
        parser = get_default_args()

        # æ·»åŠ ä¼˜åŒ–åŠŸèƒ½ç›¸å…³å‚æ•°
        if not any(arg for arg in parser._option_string_actions if arg == '--enable_optimizations'):
            parser.add_argument('--enable_optimizations', action='store_true', default=False,
                               help='å¯ç”¨ä¼˜åŒ–åŠŸèƒ½')

        # æ·»åŠ wandbé¡¹ç›®å‚æ•°
        if not any(arg for arg in parser._option_string_actions if arg == '--wandb_project'):
            parser.add_argument('--wandb_project', type=str, default='uncategorized',
                               help='WandBé¡¹ç›®åç§°ï¼ˆé»˜è®¤: uncategorizedï¼‰')

        # é»˜è®¤å¯ç”¨WandBå’Œæ•°æ®è®°å½•
        if not any(arg for arg in parser._option_string_actions if arg == '--use_wandb'):
            parser.add_argument('--use_wandb', action='store_true', default=True,
                               help='å¯ç”¨WandBè®°å½•ï¼ˆç°åœ¨é»˜è®¤å¯ç”¨ï¼‰')

        # è§£æå‚æ•°
        args = parser.parse_args()

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ”§ WandBé…ç½®æ£€æŸ¥:")
        print(f"   - use_wandb: {args.use_wandb}")
        print(f"   - ENHANCED_LOGGER_AVAILABLE: {ENHANCED_LOGGER_AVAILABLE}")
        print(f"   - wandb_project: {getattr(args, 'wandb_project', 'uncategorized')}")

        # è®¾ç½®è®¾å¤‡
        if hasattr(args, 'cuda') and args.cuda and torch.cuda.is_available():
            args.device = torch.device("cuda")
        else:
            args.device = torch.device("cpu")

        # è®¾ç½®æ™ºèƒ½ä½“æ•°é‡ï¼ˆåŸºäºç¯å¢ƒå‚æ•°ï¼‰
        args.num_agents = args.num_reservoirs + args.num_plants

        # è®¾ç½®è¿è¡Œç›®å½•
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        project_name = "water_optimized" if args.enable_optimizations else "water_standard"
        args.run_dir = f"./results/{project_name}_{timestamp}"
        os.makedirs(args.run_dir, exist_ok=True)

        # åº”ç”¨ä¼˜åŒ–è¶…å‚æ•°ï¼ˆå¦‚æœå¯ç”¨äº†ä¼˜åŒ–ï¼‰
        if args.enable_optimizations:
            if CONFIG_AVAILABLE:
                optimized_params = get_optimized_hyperparameters()

                # æ›´æ–°å…³é”®å‚æ•°
                args.hidden_size = 128
                args.entropy_coef = 0.02
                args.lr = optimized_params.get('lr', 4e-5)
                args.critic_lr = optimized_params.get('critic_lr', 1.5e-4)
                args.clip_param = optimized_params.get('clip_param', 0.05)
                args.ppo_epoch = optimized_params.get('ppo_epoch', 4)
                args.max_grad_norm = optimized_params.get('max_grad_norm', 0.5)

                print(f" åº”ç”¨ä¼˜åŒ–é…ç½®:")
                print(f"   - éšè—å±‚å¤§å°: {args.hidden_size}")
                print(f"   - å­¦ä¹ ç‡: {args.lr}")
                print(f"   - ç†µç³»æ•°: {args.entropy_coef}")
            else:
                print("ï¸ é…ç½®æ–‡ä»¶ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤ä¼˜åŒ–å‚æ•°")

        # æ˜¾ç¤ºWandBé…ç½®
        if args.use_wandb:
            print(f" WandBé…ç½®:")
            print(f"   - é¡¹ç›®å: {args.wandb_project}")

        # æ‰“å°é…ç½®æ‘˜è¦
        if CONFIG_AVAILABLE:
            print_config_summary(args)
        else:
            print(f"   - æ™ºèƒ½ä½“: {args.num_reservoirs}æ°´åº“ + {args.num_plants}æ°´å‚")
            print(f"   - Episodeé•¿åº¦: {args.episode_length}")
            print(f"   - æ€»æ­¥æ•°: {args.num_env_steps}")
            print(f"   - å­¦ä¹ ç‡: {args.lr}")
            print(f"   - è®¾å¤‡: {args.device}")

        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(args.seed)

        # å¼€å§‹è®­ç»ƒ
        print("\n å¼€å§‹è®­ç»ƒ...")
        train(args)

        print("\n è®­ç»ƒå®Œæˆ!")

    except KeyboardInterrupt:
        print("\nï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        try:
            if 'args' in locals() and args.use_wandb:
                wandb.finish()
        except:
            pass

        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        print("\n è®­ç»ƒç»“æŸ")


if __name__ == "__main__":
    main()
else:
    # å…¼å®¹ wandb agent
    print(" æ°´èµ„æºç®¡ç†è®­ç»ƒæ¨¡å—å·²å¯¼å…¥")
    print(" wandb agent è¶…å‚æ•°è°ƒä¼˜")
    