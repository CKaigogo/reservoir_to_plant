"""
ç®€åŒ–å¥–åŠ±ç³»ç»Ÿ
"""

import numpy as np
from collections import deque
from typing import Dict, Tuple, List


class SimpleRewardSystem:
    """ç®€åŒ–çš„å¥–åŠ±"""
    
    def __init__(self, n_reservoirs=12, n_plants=3, max_episode_steps=96):
        self.n_reservoirs = n_reservoirs
        self.n_plants = n_plants
        self.max_episode_steps = max_episode_steps
        
        # ç®€åŒ–çš„å¥–åŠ±æƒé‡
        self.reward_weights = {
            'supply_satisfaction': 1.0,  # é™ä½ä»2.0
            'reservoir_safety': 1.0,     # é™ä½ä»5.0  
            'ecological_balance': 0.5,   # é™ä½ä»1.0
            'stability_bonus': 0.2       # é™ä½ä»2.0
        }

        self.reward_smoother = {agent_id: deque(maxlen=5) for agent_id in self._get_all_agent_ids()}
        self.performance_history = deque(maxlen=20)
        
        # å¥–åŠ±èŒƒå›´æ§åˆ¶
        self.min_reward = -2.0
        self.max_reward = 5.0
        
        print("ç®€åŒ–å¥–åŠ±ç³»ç»Ÿå·²åˆå§‹åŒ–")
        # print(f"   - å¥–åŠ±èŒƒå›´: [{self.min_reward}, {self.max_reward}]")
        # print(f"   - æƒé‡ç®€åŒ–: {self.reward_weights}")

    def calculate_rewards(self, state: Dict, actions: Dict, current_step: int,
                         episode_progress: float = None, is_terminal: bool = False,
                         prev_state: Dict = None) -> Tuple[Dict, Dict]:
        """ç®€åŒ–çš„å¥–åŠ±è®¡ç®— - ä¸“æ³¨äºç¨³å®šæ€§"""
        
        try:
            # 1. è®¡ç®—æ ¸å¿ƒå¥–åŠ±ç»„ä»¶
            supply_rewards = self._calculate_simple_supply_rewards(state)
            safety_rewards = self._calculate_simple_safety_rewards(state)
            ecological_rewards = self._calculate_simple_ecological_rewards(state)
            stability_rewards = self._calculate_simple_stability_rewards()
            
            # 2. åˆå¹¶å¥–åŠ±ï¼ˆçº¿æ€§ç»„åˆï¼‰
            raw_rewards = {}
            for agent_id in self._get_all_agent_ids():
                raw_reward = (
                    supply_rewards.get(agent_id, 0.0) * self.reward_weights['supply_satisfaction'] +
                    safety_rewards.get(agent_id, 0.0) * self.reward_weights['reservoir_safety'] +
                    ecological_rewards.get(agent_id, 0.0) * self.reward_weights['ecological_balance'] +
                    stability_rewards.get(agent_id, 0.0) * self.reward_weights['stability_bonus']
                )
                raw_rewards[agent_id] = raw_reward
            
            # 3.åº”ç”¨å¥–åŠ±å¹³æ»‘å’ŒèŒƒå›´é™åˆ¶
            final_rewards = self._apply_smoothing_and_clipping(raw_rewards)
            
            # 4. æ›´æ–°å†å²è®°å½•
            self._update_performance_history(final_rewards, state)
            
            # 5. æ„å»ºç®€åŒ–çš„ä¿¡æ¯å­—å…¸
            info_dict = self._build_simple_info_dict(
                supply_rewards, safety_rewards, ecological_rewards, 
                stability_rewards, state, final_rewards
            )
            
            return final_rewards, info_dict
            
        except Exception as e:
            print(f" ç®€åŒ–å¥–åŠ±è®¡ç®—é”™è¯¯: {e}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤å¥–åŠ±
            return self._get_safe_default_rewards()

    def _calculate_simple_supply_rewards(self, state: Dict) -> Dict:
        """ç®€åŒ–çš„ä¾›æ°´å¥–åŠ± - çº¿æ€§è®¡ç®—"""
        rewards = {}
        
        total_supply = np.sum(state.get('actual_supply', [0]))
        total_demand = np.sum(state.get('hourly_demand', [1]))
        
        # ç®€å•çš„æ»¡è¶³ç‡è®¡ç®—
        satisfaction_rate = min(total_supply / (total_demand + 1e-8), 1.0)
        
        # çº¿æ€§å¥–åŠ±ï¼Œé¿å…éçº¿æ€§å‡½æ•°
        base_supply_reward = satisfaction_rate * 2.0 - 1.0  # èŒƒå›´[-1, 1]
        
        # åˆ†é…ç»™æ‰€æœ‰æ™ºèƒ½ä½“
        for agent_id in self._get_all_agent_ids():
            rewards[agent_id] = base_supply_reward
            
        return rewards

    def _calculate_simple_safety_rewards(self, state: Dict) -> Dict:
        """ç®€åŒ–çš„å®‰å…¨å¥–åŠ± - çº¿æ€§åˆ†æ®µå‡½æ•°"""
        rewards = {}
        
        reservoir_levels = state.get('reservoirs', np.zeros(self.n_reservoirs))
        max_reservoirs = state.get('max_reservoir', np.ones(self.n_reservoirs))
        
        for i in range(self.n_reservoirs):
            agent_id = f"reservoir_{i}"
            level_ratio = reservoir_levels[i] / (max_reservoirs[i] + 1e-8)
            
            # åˆ†æ®µçº¿æ€§å‡½æ•°
            if level_ratio < 0.2:      # å±é™©æ°´ä½
                safety_reward = -1.0
            elif level_ratio < 0.3:    # è­¦æˆ’æ°´ä½  
                safety_reward = -0.5
            elif level_ratio < 0.8:    # æ­£å¸¸æ°´ä½
                safety_reward = 0.5
            else:                      # é«˜æ°´ä½
                safety_reward = 0.0
                
            rewards[agent_id] = safety_reward
            
        # æ°´å‚è·å¾—å¹³å‡å®‰å…¨å¥–åŠ±
        avg_safety = np.mean(list(rewards.values())) if rewards else 0.0
        for i in range(self.n_plants):
            agent_id = f"plant_{i}"
            rewards[agent_id] = avg_safety
            
        return rewards

    def _calculate_simple_ecological_rewards(self, state: Dict) -> Dict:
        """ç®€åŒ–çš„ç”Ÿæ€å¥–åŠ±"""
        rewards = {}
        
        ecological_releases = state.get('ecological_releases', np.zeros(self.n_reservoirs))
        target_flows = state.get('target_ecological_flows', np.zeros(self.n_reservoirs))
        
        # ç®€å•çš„è¾¾æ ‡å¥–åŠ±
        total_compliance = 0.0
        for i in range(self.n_reservoirs):
            if target_flows[i] > 0:
                compliance = min(ecological_releases[i] / (target_flows[i] + 1e-8), 1.0)
                total_compliance += compliance
        
        avg_compliance = total_compliance / max(self.n_reservoirs, 1)
        base_eco_reward = (avg_compliance - 0.5) * 0.5  # èŒƒå›´[-0.25, 0.25]
        
        # åˆ†é…ç»™æ‰€æœ‰æ™ºèƒ½ä½“
        for agent_id in self._get_all_agent_ids():
            rewards[agent_id] = base_eco_reward
            
        return rewards

    def _calculate_simple_stability_rewards(self) -> Dict:
        """ç®€åŒ–çš„ç¨³å®šæ€§å¥–åŠ±"""
        rewards = {}
        
        # ğŸ¯ åŸºäºå†å²æ€§èƒ½çš„ç¨³å®šæ€§å¥–åŠ±
        if len(self.performance_history) >= 5:
            recent_performance = list(self.performance_history)[-5:]
            stability = 1.0 / (1.0 + np.std(recent_performance))
            stability_reward = (stability - 0.5) * 0.2  # å°å¹…å¥–åŠ±
        else:
            stability_reward = 0.0
            
        # åˆ†é…ç»™æ‰€æœ‰æ™ºèƒ½ä½“
        for agent_id in self._get_all_agent_ids():
            rewards[agent_id] = stability_reward
            
        return rewards

    def _apply_smoothing_and_clipping(self, raw_rewards: Dict) -> Dict:
        """åº”ç”¨å¹³æ»‘å’ŒèŒƒå›´é™åˆ¶"""
        final_rewards = {}
        
        for agent_id, raw_reward in raw_rewards.items():
            # 1. æ·»åŠ åˆ°å¹³æ»‘å™¨
            self.reward_smoother[agent_id].append(raw_reward)
            
            # 2. è®¡ç®—å¹³æ»‘å¥–åŠ±ï¼ˆç§»åŠ¨å¹³å‡ï¼‰
            if len(self.reward_smoother[agent_id]) >= 3:
                smoothed_reward = np.mean(list(self.reward_smoother[agent_id]))
            else:
                smoothed_reward = raw_reward
            
            # 3.ä¸¥æ ¼çš„èŒƒå›´é™åˆ¶
            clipped_reward = np.clip(smoothed_reward, self.min_reward, self.max_reward)
            
            final_rewards[agent_id] = clipped_reward
            
        return final_rewards

    def _update_performance_history(self, rewards: Dict, state: Dict):
        """æ›´æ–°æ€§èƒ½å†å²"""
        avg_reward = np.mean(list(rewards.values())) if rewards else 0.0
        self.performance_history.append(avg_reward)

    def _get_all_agent_ids(self) -> List[str]:
        """è·å–æ‰€æœ‰æ™ºèƒ½ä½“ID"""
        agent_ids = []
        for i in range(self.n_reservoirs):
            agent_ids.append(f"reservoir_{i}")
        for i in range(self.n_plants):
            agent_ids.append(f"plant_{i}")
        return agent_ids

    def _build_simple_info_dict(self, supply_rewards, safety_rewards, ecological_rewards, 
                               stability_rewards, state, final_rewards):
        """æ„å»ºç®€åŒ–çš„ä¿¡æ¯å­—å…¸"""
        
        # è®¡ç®—å…¨å±€æŒ‡æ ‡
        total_supply = np.sum(state.get('actual_supply', [0]))
        total_demand = np.sum(state.get('hourly_demand', [1]))
        supply_satisfaction = min(total_supply / (total_demand + 1e-8), 1.0)
        
        reservoir_levels = state.get('reservoirs', np.zeros(self.n_reservoirs))
        max_reservoirs = state.get('max_reservoir', np.ones(self.n_reservoirs))
        avg_reservoir_level = np.mean(reservoir_levels / (max_reservoirs + 1e-8))
        
        # ç®€åŒ–çš„ä¿¡æ¯å­—å…¸
        base_info = {
            'supply_satisfaction_rate': supply_satisfaction,
            'avg_reservoir_level': avg_reservoir_level,
            'reward_components': {
                'supply': np.mean(list(supply_rewards.values())) if supply_rewards else 0.0,
                'safety': np.mean(list(safety_rewards.values())) if safety_rewards else 0.0,
                'ecological': np.mean(list(ecological_rewards.values())) if ecological_rewards else 0.0,
                'stability': np.mean(list(stability_rewards.values())) if stability_rewards else 0.0
            },
            'reward_range': f"[{self.min_reward}, {self.max_reward}]",
            'system_type': 'simplified_reward_system'
        }
        
        # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“å¤åˆ¶åŸºç¡€ä¿¡æ¯
        info_dict = {}
        for agent_id in self._get_all_agent_ids():
            agent_info = base_info.copy()
            agent_info['total_reward'] = final_rewards.get(agent_id, 0.0)
            info_dict[agent_id] = agent_info
            
        return info_dict

    def _get_safe_default_rewards(self):
        """è·å–å®‰å…¨çš„é»˜è®¤å¥–åŠ±"""
        default_rewards = {agent_id: 0.0 for agent_id in self._get_all_agent_ids()}
        default_info = {agent_id: {'total_reward': 0.0, 'system_type': 'simplified_reward_system'} 
                       for agent_id in self._get_all_agent_ids()}
        return default_rewards, default_info 