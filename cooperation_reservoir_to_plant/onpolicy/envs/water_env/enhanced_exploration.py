"""
æ¢ç´¢æœºåˆ¶
"""

import numpy as np
from collections import deque, defaultdict
from typing import Dict, List, Tuple


class EnhancedExplorationManager:
    """æ¢ç´¢ç®¡ç†å™¨ - æ¢ç´¢ç­–ç•¥"""
    
    def __init__(self, n_reservoirs=4, n_plants=1, max_episode_steps=168):
        self.n_reservoirs = n_reservoirs
        self.n_plants = n_plants
        self.max_episode_steps = max_episode_steps
        
        # ğŸ¯ æ¢ç´¢å‚æ•°
        self.exploration_config = {
            'initial_exploration_rate': 0.3,
            'min_exploration_rate': 0.05,
            'decay_episodes': 500,
            'diversity_threshold': 0.1,
            'reward_threshold': 0.5
        }
        

        self.exploration_state = {
            'current_episode': 0,
            'exploration_active': True,
            'exploration_rate': self.exploration_config['initial_exploration_rate'],
            'phase': 'exploration'  # exploration, transition, exploitation
        }
        
        #åŠ¨ä½œå†å²è®°å½•
        self.action_history = {agent_id: deque(maxlen=100) for agent_id in self._get_all_agent_ids()}
        self.performance_history = deque(maxlen=50)
        self.diversity_history = deque(maxlen=30)
        
        # æ¢ç´¢å¥–åŠ±é…ç½®
        self.exploration_rewards = {
            'diversity_bonus': 0.1,      # å¤šæ ·æ€§å¥–åŠ±
            'novelty_bonus': 0.05,       # æ–°é¢–æ€§å¥–åŠ±  
            'progress_bonus': 0.02       # è¿›æ­¥å¥–åŠ±
        }
        
        print(" æ¢ç´¢æœºåˆ¶å·²åˆå§‹åŒ–")
        # print(f"   - åˆå§‹æ¢ç´¢ç‡: {self.exploration_config['initial_exploration_rate']}")
        # print(f"   - å¤šæ ·æ€§é˜ˆå€¼: {self.exploration_config['diversity_threshold']}")

    def update_exploration_state(self, episode: int, episode_rewards: Dict, episode_performance: Dict):
        """æ›´æ–°æ¢ç´¢çŠ¶æ€"""
        self.exploration_state['current_episode'] = episode
        
        # æ›´æ–°å†å²è®°å½•
        avg_reward = np.mean(list(episode_rewards.values())) if episode_rewards else 0.0
        self.performance_history.append(avg_reward)

        current_diversity = self._calculate_current_diversity()
        self.diversity_history.append(current_diversity)
        
        # æ›´æ–°æ¢ç´¢ç‡ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰
        decay_factor = max(0, (self.exploration_config['decay_episodes'] - episode) / self.exploration_config['decay_episodes'])
        self.exploration_state['exploration_rate'] = (
            self.exploration_config['min_exploration_rate'] + 
            (self.exploration_config['initial_exploration_rate'] - self.exploration_config['min_exploration_rate']) * decay_factor
        )

        self._update_exploration_phase()
        
        return self.exploration_state

    def calculate_exploration_rewards(self, actions: Dict, state: Dict) -> Dict:
        """è®¡ç®—æ¢ç´¢å¥–åŠ±"""
        exploration_rewards = {}
        
        for agent_id in self._get_all_agent_ids():
            total_exploration_reward = 0.0
            
            # 1. å¤šæ ·æ€§å¥–åŠ±
            diversity_reward = self._calculate_diversity_reward(agent_id, actions.get(agent_id, {}))
            total_exploration_reward += diversity_reward * self.exploration_rewards['diversity_bonus']
            
            # 2. æ–°é¢–æ€§å¥–åŠ±
            novelty_reward = self._calculate_novelty_reward(agent_id, actions.get(agent_id, {}))
            total_exploration_reward += novelty_reward * self.exploration_rewards['novelty_bonus']
            
            # 3. è¿›æ­¥å¥–åŠ±
            progress_reward = self._calculate_progress_reward()
            total_exploration_reward += progress_reward * self.exploration_rewards['progress_bonus']
            
            # 4. åº”ç”¨æ¢ç´¢æƒé‡
            final_exploration_reward = total_exploration_reward * self.exploration_state['exploration_rate']
            
            exploration_rewards[agent_id] = final_exploration_reward
            
        # è®°å½•åŠ¨ä½œå†å²
        self._record_actions(actions)
        
        return exploration_rewards

    def _calculate_diversity_reward(self, agent_id: str, action: Dict) -> float:
        """è®¡ç®—å¤šæ ·æ€§å¥–åŠ±"""
        if not action or len(self.action_history[agent_id]) < 5:
            return 0.0
        
        # æå–åŠ¨ä½œç‰¹å¾
        current_features = self._extract_action_features(agent_id, action)
        
        # ä¸å†å²åŠ¨ä½œæ¯”è¾ƒ
        recent_actions = list(self.action_history[agent_id])[-10:]
        if not recent_actions:
            return 1.0  # é¦–æ¬¡åŠ¨ä½œç»™äºˆæ»¡åˆ†
        
        # è®¡ç®—ä¸æœ€è¿‘åŠ¨ä½œçš„å·®å¼‚åº¦
        diversity_scores = []
        for past_features in recent_actions:
            if past_features is not None:
                # è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»
                diff = np.linalg.norm(current_features - past_features)
                diversity_scores.append(diff)
        
        if diversity_scores:
            avg_diversity = np.mean(diversity_scores)
            # å½’ä¸€åŒ–åˆ°[0, 1]èŒƒå›´
            normalized_diversity = min(avg_diversity / 2.0, 1.0)
            return normalized_diversity
        
        return 0.0

    def _calculate_novelty_reward(self, agent_id: str, action: Dict) -> float:
        """è®¡ç®—æ–°é¢–æ€§å¥–åŠ±"""
        if not action:
            return 0.0
        
        current_features = self._extract_action_features(agent_id, action)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå…¨æ–°çš„åŠ¨ä½œç»„åˆ
        all_history = list(self.action_history[agent_id])
        if not all_history:
            return 1.0  # é¦–æ¬¡åŠ¨ä½œ
        
        # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„å†å²åŠ¨ä½œ
        min_distance = float('inf')
        for past_features in all_history:
            if past_features is not None:
                distance = np.linalg.norm(current_features - past_features)
                min_distance = min(min_distance, distance)
        
        # å¦‚æœè·ç¦»è¶³å¤Ÿå¤§ï¼Œè®¤ä¸ºæ˜¯æ–°é¢–åŠ¨ä½œ
        novelty_threshold = 0.3
        if min_distance > novelty_threshold:
            return min(min_distance / 1.0, 1.0)  # å½’ä¸€åŒ–
        
        return 0.0

    def _calculate_progress_reward(self) -> float:
        """è®¡ç®—è¿›æ­¥å¥–åŠ±"""
        if len(self.performance_history) < 10:
            return 0.0
        
        # æ¯”è¾ƒæœ€è¿‘å’Œæ—©æœŸçš„æ€§èƒ½
        recent_performance = np.mean(list(self.performance_history)[-5:])
        early_performance = np.mean(list(self.performance_history)[:5])
        
        improvement = recent_performance - early_performance
        # å½’ä¸€åŒ–æ”¹è¿›ç¨‹åº¦
        progress_reward = np.tanh(improvement)
        
        return max(progress_reward, 0.0)

    def _extract_action_features(self, agent_id: str, action: Dict) -> np.ndarray:
        """æå–åŠ¨ä½œç‰¹å¾å‘é‡"""
        if agent_id.startswith('reservoir_'):
            # æ°´åº“åŠ¨ä½œç‰¹å¾
            release_ratio = action.get('total_release_ratio', [0.1])[0]
            allocation_weights = action.get('allocation_weights', [1.0])
            emergency_release = action.get('emergency_release', 0)
            
            # æ„é€ ç‰¹å¾å‘é‡
            features = [release_ratio, emergency_release]
            if isinstance(allocation_weights, (list, np.ndarray)):
                features.extend(allocation_weights[:3])
            else:
                features.append(allocation_weights)
            
            return np.array(features[:5])
        
        else:  # plant
            # æ°´å‚åŠ¨ä½œç‰¹å¾
            demand_adjustment = action.get('demand_adjustment', [1.0])[0]
            priority_level = action.get('priority_level', 1)
            storage_strategy = action.get('storage_strategy', [0.5])[0]
            
            return np.array([demand_adjustment, priority_level, storage_strategy, 0.0, 0.0])  # å¡«å……åˆ°5ç»´

    def _calculate_current_diversity(self) -> float:
        """è®¡ç®—å½“å‰ç³»ç»Ÿçš„æ•´ä½“å¤šæ ·æ€§"""
        all_diversities = []
        
        for agent_id in self._get_all_agent_ids():
            agent_history = list(self.action_history[agent_id])
            if len(agent_history) >= 5:
                recent_features = agent_history[-5:]
                # è®¡ç®—å†…éƒ¨å¤šæ ·æ€§
                diversity_scores = []
                for i in range(len(recent_features)):
                    for j in range(i+1, len(recent_features)):
                        if recent_features[i] is not None and recent_features[j] is not None:
                            diff = np.linalg.norm(recent_features[i] - recent_features[j])
                            diversity_scores.append(diff)
                
                if diversity_scores:
                    agent_diversity = np.mean(diversity_scores)
                    all_diversities.append(agent_diversity)
        
        return np.mean(all_diversities) if all_diversities else 0.0

    def _update_exploration_phase(self):
        """æ›´æ–°æ¢ç´¢é˜¶æ®µ"""
        episode = self.exploration_state['current_episode']
        exploration_rate = self.exploration_state['exploration_rate']
        
        # æ ¹æ®episodeæ•°é‡å’Œæ€§èƒ½ç¡®å®šé˜¶æ®µ
        if episode < 200:
            self.exploration_state['phase'] = 'exploration'
            self.exploration_state['exploration_active'] = True
        elif episode < 800:
            self.exploration_state['phase'] = 'transition'
            self.exploration_state['exploration_active'] = True
        else:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­æ¢ç´¢
            if len(self.performance_history) >= 20:
                recent_performance = list(self.performance_history)[-10:]
                performance_std = np.std(recent_performance)
                
                if performance_std < 0.1:  # æ€§èƒ½ç¨³å®šï¼Œå‡å°‘æ¢ç´¢
                    self.exploration_state['phase'] = 'exploitation'
                    self.exploration_state['exploration_active'] = False
                else:
                    self.exploration_state['phase'] = 'transition'
                    self.exploration_state['exploration_active'] = True
            else:
                self.exploration_state['phase'] = 'transition'
                self.exploration_state['exploration_active'] = True

    def _record_actions(self, actions: Dict):
        """è®°å½•åŠ¨ä½œå†å²"""
        for agent_id, action in actions.items():
            if agent_id in self.action_history:
                features = self._extract_action_features(agent_id, action)
                self.action_history[agent_id].append(features)

    def _get_all_agent_ids(self) -> List[str]:
        """è·å–æ‰€æœ‰æ™ºèƒ½ä½“ID"""
        agent_ids = []
        for i in range(self.n_reservoirs):
            agent_ids.append(f"reservoir_{i}")
        for i in range(self.n_plants):
            agent_ids.append(f"plant_{i}")
        return agent_ids

    def get_exploration_summary(self) -> Dict:
        """è·å–æ¢ç´¢çŠ¶æ€æ‘˜è¦"""
        return {
            'current_state': self.exploration_state.copy(),
            'metrics': {
                'current_diversity': self._calculate_current_diversity(),
                'avg_performance': np.mean(list(self.performance_history)) if self.performance_history else 0.0,
                'exploration_effectiveness': self._calculate_exploration_effectiveness()
            }
        }

    def _calculate_exploration_effectiveness(self) -> float:
        """è®¡ç®—æ¢ç´¢æœ‰æ•ˆæ€§"""
        if len(self.diversity_history) < 5 or len(self.performance_history) < 5:
            return 0.5  # é»˜è®¤å€¼
        
        # å¤šæ ·æ€§è¶‹åŠ¿
        recent_diversity = np.mean(list(self.diversity_history)[-5:])
        early_diversity = np.mean(list(self.diversity_history)[:5])
        diversity_trend = recent_diversity - early_diversity
        
        # æ€§èƒ½è¶‹åŠ¿
        recent_performance = np.mean(list(self.performance_history)[-10:])
        early_performance = np.mean(list(self.performance_history)[:10])
        performance_trend = recent_performance - early_performance
        
        # ç»¼åˆè¯„ä¼°ï¼šå¤šæ ·æ€§ä¸æ€§èƒ½æå‡
        effectiveness = 0.6 * performance_trend + 0.4 * diversity_trend
        return np.clip(effectiveness, 0.0, 1.0) 