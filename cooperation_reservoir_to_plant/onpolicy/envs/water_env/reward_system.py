"""
å¥–åŠ±ç³»ç»Ÿ - åº”ç”¨åŠ¿å‡½æ•°å¥–åŠ±å¡‘å½¢å’Œç¨³å®šæ€§æ”¹è¿›
åŸºäºPBRSç†è®º
"""

import numpy as np
from collections import deque, defaultdict
from typing import Dict, Tuple, List, Any, Optional
import math


class ExplorationRecorder:
    """æ¢ç´¢è®°å½•å™¨ - è¿½è¸ªå’Œå¥–åŠ±æ¢ç´¢è¡Œä¸º"""
    
    def __init__(self, n_reservoirs=12, n_plants=3, max_episode_steps=168):
        self.n_reservoirs = n_reservoirs
        self.n_plants = n_plants
        self.max_episode_steps = max_episode_steps
        
        # æ¢ç´¢çŠ¶æ€
        self.exploration_active = True
        self.exploration_phase = 'initial'
        self.episode_count = 0
        self.exploration_effectiveness = 0.0
        
        # åŠ¨ä½œå¤šæ ·æ€§è¿½è¸ª
        self.action_history = defaultdict(lambda: deque(maxlen=50))
        self.action_diversity_scores = defaultdict(float)
        
        # çŠ¶æ€æ¢ç´¢è¿½è¸ª
        self.state_visit_counts = defaultdict(int)
        self.recent_discoveries = deque(maxlen=20)
        
        # æ¢ç´¢å¥–åŠ±å‚æ•°
        self.exploration_bonus_scale = 1.0
        self.diversity_threshold = 0.3
        self.discovery_bonus = 0.5
        
        # æ¢ç´¢æŒ‡æ ‡
        self.exploration_metrics = {
            'discovery_rate': 0.0,
            'convergence_speed': 0.0,
            'stability_progression': 0.0,
            'reward_improvement': 0.0
        }
        
        print("æ¢ç´¢è®°å½•å™¨å·²åˆå§‹åŒ–")
    
    def record_episode_start(self):
        """è®°å½•æ–°Episodeå¼€å§‹"""
        self.episode_count += 1
        
        # æ›´æ–°æ¢ç´¢é˜¶æ®µ
        if self.episode_count < 100:
            self.exploration_phase = 'initial'
            self.exploration_active = True
        elif self.episode_count < 500:
            self.exploration_phase = 'expansion'
            self.exploration_active = True
        elif self.episode_count < 1000:
            self.exploration_phase = 'refinement'
            self.exploration_active = True
        else:
            self.exploration_phase = 'exploitation'
            self.exploration_active = False

        # å¼ºåˆ¶ç»ˆæ­¢æ¢ç´¢çš„æ¡ä»¶
        if self.episode_count > 1500:  # ç¡¬ä¸Šé™ï¼š1500ä¸ªepisodeåå¼ºåˆ¶åœæ­¢æ¢ç´¢
            self.exploration_active = False
            self.exploration_phase = 'forced_exploitation'
    
    def record_actions(self, actions: Dict, state: Dict) -> Dict:
        """è®°å½•åŠ¨ä½œå¹¶è®¡ç®—æ¢ç´¢å¥–åŠ±"""
        exploration_rewards = {}
        
        if not self.exploration_active:
            # æ¢ç´¢é˜¶æ®µç»“æŸï¼Œè¿”å›0å¥–åŠ±
            return {agent_id: 0.0 for agent_id in self._get_all_agent_ids()}
        
        # è®¡ç®—åŠ¨ä½œå¤šæ ·æ€§
        for agent_id, action in actions.items():
            if isinstance(action, dict):
                # å°†åŠ¨ä½œè½¬æ¢ä¸ºç‰¹å¾å‘é‡
                action_features = self._extract_action_features(action, agent_id)
                
                # è®°å½•åŠ¨ä½œå†å²
                self.action_history[agent_id].append(action_features)
                
                # è®¡ç®—å¤šæ ·æ€§åˆ†æ•°
                diversity_score = self._calculate_action_diversity(agent_id)
                self.action_diversity_scores[agent_id] = diversity_score
                
                # è®¡ç®—æ¢ç´¢å¥–åŠ±
                exploration_reward = self._calculate_exploration_reward(agent_id, diversity_score, state)
                exploration_rewards[agent_id] = exploration_reward
        
        # è®°å½•çŠ¶æ€è®¿é—®
        state_key = self._get_state_key(state)
        self.state_visit_counts[state_key] += 1
        
        # å‘ç°æ–°çŠ¶æ€çš„å¥–åŠ±
        if self.state_visit_counts[state_key] == 1:
            self.recent_discoveries.append(state_key)
            # ç»™æ‰€æœ‰æ™ºèƒ½ä½“å‘ç°å¥–åŠ±
            for agent_id in exploration_rewards:
                exploration_rewards[agent_id] += self.discovery_bonus
        
        return exploration_rewards
    
    def _extract_action_features(self, action: Dict, agent_id: str) -> np.ndarray:
        """æå–åŠ¨ä½œç‰¹å¾å‘é‡"""
        if agent_id.startswith('reservoir'):
            # æ°´åº“åŠ¨ä½œç‰¹å¾
            release_ratio = action.get('total_release_ratio', [0.0])[0]
            weights = action.get('allocation_weights', [1.0])
            emergency = action.get('emergency_release', 0)
            
            features = [release_ratio, np.mean(weights), np.std(weights), float(emergency)]
        else:
            # æ°´å‚åŠ¨ä½œç‰¹å¾
            demand_adj = action.get('demand_adjustment', [1.0])[0]
            priority = action.get('priority_level', 1)
            storage = action.get('storage_strategy', [0.5])[0]
            
            features = [demand_adj, float(priority), storage]
        
        return np.array(features, dtype=np.float32)
    
    def _calculate_action_diversity(self, agent_id: str) -> float:
        """è®¡ç®—åŠ¨ä½œå¤šæ ·æ€§åˆ†æ•°"""
        if len(self.action_history[agent_id]) < 2:
            return 0.0
        
        # è®¡ç®—æœ€è¿‘åŠ¨ä½œçš„æ ‡å‡†å·®
        recent_actions = list(self.action_history[agent_id])[-10:]
        action_matrix = np.array(recent_actions)
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„æ ‡å‡†å·®
        feature_stds = np.std(action_matrix, axis=0)
        
        # å¹³å‡æ ‡å‡†å·®ä½œä¸ºå¤šæ ·æ€§åˆ†æ•°
        diversity_score = np.mean(feature_stds)
        
        return diversity_score
    
    def _calculate_exploration_reward(self, agent_id: str, diversity_score: float, state: Dict) -> float:
        """è®¡ç®—æ¢ç´¢å¥–åŠ±"""
        if not self.exploration_active:
            return 0.0
        
        # åŸºç¡€å¤šæ ·æ€§å¥–åŠ±
        diversity_bonus = 0.0
        if diversity_score > self.diversity_threshold:
            diversity_bonus = self.exploration_bonus_scale * (diversity_score - self.diversity_threshold)
        
        # åŸºäºå½“å‰çŠ¶æ€çš„æ¢ç´¢å¥–åŠ±
        state_bonus = 0.0
        if agent_id.startswith('reservoir'):
            # æ°´åº“æ¢ç´¢å¥–åŠ±ï¼šé¼“åŠ±åœ¨ä¸åŒæ°´ä½ä¸‹å°è¯•ä¸åŒç­–ç•¥
            reservoir_idx = int(agent_id.split('_')[1])
            if reservoir_idx < len(state['reservoirs']):
                level_ratio = state['reservoirs'][reservoir_idx] / state['max_reservoir'][reservoir_idx]
                # åœ¨å±é™©æ°´ä½ä¸‹æ¢ç´¢æ›´æœ‰ä»·å€¼
                if level_ratio < 0.4:
                    state_bonus = 0.2
                elif level_ratio > 0.8:
                    state_bonus = 0.1
        
        total_exploration_reward = diversity_bonus + state_bonus
        
        # æ ¹æ®æ¢ç´¢é˜¶æ®µè°ƒæ•´å¥–åŠ±
        phase_multiplier = {
            'initial': 1.0,
            'expansion': 0.8,
            'refinement': 0.5,
            'exploitation': 0.1
        }
        
        total_exploration_reward *= phase_multiplier.get(self.exploration_phase, 0.5)
        
        return total_exploration_reward
    
    def _get_state_key(self, state: Dict) -> str:
        """è·å–çŠ¶æ€çš„å”¯ä¸€é”®"""
        # ç®€åŒ–çŠ¶æ€è¡¨ç¤º
        reservoir_levels = state['reservoirs'] / state['max_reservoir']
        supply_ratios = state['actual_supply'] / (state['hourly_demand'] + 1e-8)
        
        # ç¦»æ•£åŒ–çŠ¶æ€
        discrete_levels = np.round(reservoir_levels, 1)
        discrete_supply = np.round(supply_ratios, 1)
        
        return f"r{discrete_levels.tolist()}_s{discrete_supply.tolist()}"
    
    def update_exploration_effectiveness(self, rewards: Dict, state: Dict):
        """æ›´æ–°æ¢ç´¢æ•ˆæœ"""
        # ç®€å•çš„æ¢ç´¢æ•ˆæœè¯„ä¼°
        avg_reward = np.mean(list(rewards.values()))
        diversity_score = np.mean(list(self.action_diversity_scores.values()))
        
        # ç»“åˆå¥–åŠ±å’Œå¤šæ ·æ€§
        self.exploration_effectiveness = 0.7 * avg_reward + 0.3 * diversity_score
        
        # æ›´æ–°æ¢ç´¢æŒ‡æ ‡
        self.exploration_metrics['discovery_rate'] = len(self.recent_discoveries) / 20.0
        self.exploration_metrics['convergence_speed'] = min(1.0, self.episode_count / 1000.0)
        self.exploration_metrics['stability_progression'] = self.exploration_effectiveness
        self.exploration_metrics['reward_improvement'] = max(0, avg_reward)
    
    def get_exploration_summary(self) -> Dict:
        """è·å–æ¢ç´¢æ€»ç»“"""
        return {
            'current_state': {
                'phase': self.exploration_phase,
                'exploration_active': self.exploration_active,
                'episode_count': self.episode_count,
                'exploration_effectiveness': self.exploration_effectiveness
            },
            'metrics': self.exploration_metrics
        }
    
    def get_agent_exploration_profiles(self) -> Dict:
        """è·å–æ™ºèƒ½ä½“æ¢ç´¢æ¡£æ¡ˆ"""
        profiles = {}
        
        for agent_id in self._get_all_agent_ids():
            diversity_score = self.action_diversity_scores.get(agent_id, 0.0)
            
            # æ ¹æ®å¤šæ ·æ€§åˆ†æ•°åˆ†ç±»ç­–ç•¥
            if diversity_score > 0.5:
                strategy = 'explorer'
            elif diversity_score > 0.3:
                strategy = 'balanced'
            else:
                strategy = 'conservative'
            
            profiles[agent_id] = {
                'dominant_strategy': strategy,
                'risk_level': diversity_score,
                'recent_performance': self.exploration_effectiveness
            }
        
        return profiles
    
    def _get_all_agent_ids(self) -> List[str]:
        """è·å–æ‰€æœ‰æ™ºèƒ½ä½“ID"""
        agent_ids = []
        for i in range(self.n_reservoirs):
            agent_ids.append(f"reservoir_{i}")
        for i in range(self.n_plants):
            agent_ids.append(f"plant_{i}")
        return agent_ids


class PotentialBasedRewardShaper:
    """åŠ¿å‡½æ•°å¥–åŠ±å¡‘å½¢å™¨ - è§£å†³æ”¶æ•›é—®é¢˜çš„å…³é”®ç»„ä»¶"""

    def __init__(self, n_reservoirs, n_plants, gamma=0.995):
        self.n_reservoirs = n_reservoirs
        self.n_plants = n_plants
        self.gamma = gamma

        # åŠ¿å‡½æ•°å†å²
        self.prev_potentials = {}

        # å¤šå±‚æ¬¡åŠ¿å‡½æ•°è®¾è®¡
        self.potential_weights = {
            'supply_potential': 2.0,  # ä¾›æ°´æ»¡è¶³åŠ¿èƒ½
            'safety_potential': 6.0,  # å®‰å…¨åŠ¿èƒ½
            'ecological_potential': 2.0  # ç”Ÿæ€åŠ¿èƒ½
        }

    def calculate_potential_shaping(self, state: Dict, prev_state: Dict = None) -> Dict:
        """è®¡ç®—åŠ¿å‡½æ•°å¥–åŠ±å¡‘å½¢"""
        current_potentials = self._calculate_potentials(state)

        if prev_state is None or not self.prev_potentials:
            # åˆå§‹çŠ¶æ€ï¼Œæ— å¡‘å½¢å¥–åŠ±
            shaping_rewards = {agent_id: 0.0 for agent_id in self._get_all_agent_ids()}
        else:
            # PBRSå…¬å¼ï¼šF(s,a,s') = Î³*Î¦(s') - Î¦(s)
            shaping_rewards = {}
            for agent_id in self._get_all_agent_ids():
                current_phi = current_potentials.get(agent_id, 0.0)
                prev_phi = self.prev_potentials.get(agent_id, 0.0)
                shaping_rewards[agent_id] = self.gamma * current_phi - prev_phi

        # ä¿å­˜å½“å‰åŠ¿èƒ½
        self.prev_potentials = current_potentials.copy()

        return shaping_rewards

    def _calculate_potentials(self, state: Dict) -> Dict:
        """è®¡ç®—å¤šå±‚æ¬¡åŠ¿å‡½æ•°"""
        potentials = {}

        # 1. ä¾›æ°´æ»¡è¶³åŠ¿èƒ½
        supply_potentials = self._calculate_supply_potential(state)

        # 2. å®‰å…¨åŠ¿èƒ½
        safety_potentials = self._calculate_safety_potential(state)

        # 3. ç”Ÿæ€åŠ¿èƒ½
        ecological_potentials = self._calculate_ecological_potential(state)

        # åˆå¹¶åŠ¿èƒ½
        for agent_id in self._get_all_agent_ids():
            total_potential = (
                    supply_potentials.get(agent_id, 0.0) * self.potential_weights['supply_potential'] +
                    safety_potentials.get(agent_id, 0.0) * self.potential_weights['safety_potential'] +
                    ecological_potentials.get(agent_id, 0.0) * self.potential_weights['ecological_potential']
            )
            potentials[agent_id] = total_potential

        return potentials

    def _calculate_supply_potential(self, state: Dict) -> Dict:
        """è®¡ç®—ä¾›æ°´æ»¡è¶³åŠ¿å‡½æ•° - éçº¿æ€§åŠ¿èƒ½"""
        potentials = {}

        # å…¨å±€ä¾›æ°´æ»¡è¶³ç‡
        total_supply = np.sum(state['actual_supply'])
        total_demand = np.sum(state['hourly_demand'])
        global_satisfaction = min(total_supply / (total_demand + 1e-8), 1.0)

        # ä½¿ç”¨sigmoidåŠ¿å‡½æ•°-æä¾›æ›´å¹³æ»‘çš„æ¢¯åº¦
        def sigmoid_potential(x, steepness=4.0, midpoint=0.8):
            """Så‹åŠ¿å‡½æ•°ï¼Œåœ¨ç›®æ ‡é™„è¿‘æä¾›å¼ºæ¢¯åº¦"""
            return 2.0 / (1.0 + np.exp(-steepness * (x - midpoint))) - 1.0

        base_potential = sigmoid_potential(global_satisfaction)

        # ä¸ºä¸åŒç±»å‹æ™ºèƒ½ä½“åˆ†é…åŠ¿èƒ½
        for i in range(self.n_reservoirs):
            agent_id = f"reservoir_{i}"
            # æ°´åº“åŸºäºæ”¯æŒèƒ½åŠ›çš„åŠ¿èƒ½
            reservoir_level = state['reservoirs'][i] / state['max_reservoir'][i]
            level_potential = sigmoid_potential(reservoir_level, steepness=3.0, midpoint=0.6)
            potentials[agent_id] = base_potential * 0.7 + level_potential * 0.3

        for i in range(self.n_plants):
            agent_id = f"plant_{i}"
            # æ°´å‚åŸºäºéœ€æ±‚æ»¡è¶³çš„åŠ¿èƒ½
            individual_satisfaction = min(
                state['actual_supply'][i] / (state['hourly_demand'][i] + 1e-8), 1.0
            )
            individual_potential = sigmoid_potential(individual_satisfaction)
            potentials[agent_id] = base_potential * 0.5 + individual_potential * 0.5

        return potentials

    def _calculate_safety_potential(self, state: Dict) -> Dict:
        """è®¡ç®—å®‰å…¨åŠ¿å‡½æ•°"""
        potentials = {}

        def safety_potential_function(level_ratio):
            """å®‰å…¨åŠ¿å‡½æ•°ï¼šåœ¨0.25-0.75èŒƒå›´å†…ä¸ºæ­£ï¼Œå…¶ä»–ä¸ºè´Ÿ"""
            optimal_center = 0.5
            safe_width = 0.25

            if 0.25 <= level_ratio <= 0.75:  # æ‰©å¤§å®‰å…¨èŒƒå›´
                distance_from_center = abs(level_ratio - optimal_center)
                return 3.0 - (distance_from_center / safe_width) ** 2
            else:
                # å±é™©åŒºåŸŸçš„åŠ¿èƒ½æ€¥å‰§ä¸‹é™
                if level_ratio < 0.25:
                    return -5.0 * (0.25 - level_ratio) / 0.3
                else:
                    return -5.0 * (level_ratio - 0.75) / 0.2

        # è®¡ç®—æ¯ä¸ªæ°´åº“çš„å®‰å…¨åŠ¿èƒ½
        reservoir_safety_potentials = []
        for i in range(self.n_reservoirs):
            level_ratio = state['reservoirs'][i] / state['max_reservoir'][i]
            safety_pot = safety_potential_function(level_ratio)
            reservoir_safety_potentials.append(safety_pot)

        # å…¨å±€å®‰å…¨åŠ¿èƒ½
        global_safety_potential = np.mean(reservoir_safety_potentials)

        # åˆ†é…ç»™æ™ºèƒ½ä½“
        for i in range(self.n_reservoirs):
            agent_id = f"reservoir_{i}"
            # æ°´åº“ï¼šä¸ªä½“å®‰å…¨åŠ¿èƒ½ + å…¨å±€å®‰å…¨åŠ¿èƒ½
            potentials[agent_id] = reservoir_safety_potentials[i] * 0.7 + global_safety_potential * 0.3

        for i in range(self.n_plants):
            agent_id = f"plant_{i}"
            # æ°´å‚ï¼šä¸»è¦å…³æ³¨å…¨å±€å®‰å…¨
            potentials[agent_id] = global_safety_potential

        return potentials

    def _calculate_ecological_potential(self, state: Dict) -> Dict:
        """è®¡ç®—ç”Ÿæ€åŠ¿å‡½æ•°"""
        potentials = {}

        target_flows = state.get('target_ecological_flows', np.zeros(self.n_reservoirs))
        actual_releases = state.get('ecological_releases', np.zeros(self.n_reservoirs))

        ecological_potentials = []
        for i in range(self.n_reservoirs):
            if target_flows[i] > 0:
                # ç”Ÿæ€æµé‡æ»¡è¶³ç‡
                eco_satisfaction = min(actual_releases[i] / target_flows[i], 2.0)  # å…è®¸è¶…é¢

                # ç”Ÿæ€åŠ¿å‡½æ•°ï¼šåœ¨0.8-1.2èŒƒå›´å†…ä¸ºæ­£
                if 0.8 <= eco_satisfaction <= 1.2:
                    eco_potential = 1.0 - abs(eco_satisfaction - 1.0) / 0.2
                else:
                    # è¶…å‡ºåˆç†èŒƒå›´çš„æƒ©ç½š
                    if eco_satisfaction < 0.8:
                        deficit = (0.8 - eco_satisfaction) / 0.8
                    else:  # eco_satisfaction > 1.2
                        deficit = (eco_satisfaction - 1.2) / 0.8
                    eco_potential = -deficit ** 2

                ecological_potentials.append(eco_potential)
            else:
                ecological_potentials.append(0.0)

        global_eco_potential = np.mean(ecological_potentials) if ecological_potentials else 0.0

        # åªæœ‰æ°´åº“æ‰¿æ‹…ç”Ÿæ€è´£ä»»
        for i in range(self.n_reservoirs):
            agent_id = f"reservoir_{i}"
            potentials[agent_id] = ecological_potentials[i] * 0.6 + global_eco_potential * 0.4

        for i in range(self.n_plants):
            agent_id = f"plant_{i}"
            potentials[agent_id] = global_eco_potential * 0.2  # æ°´å‚è¾ƒå°çš„ç”Ÿæ€è´£ä»»

        return potentials

    def _get_all_agent_ids(self) -> List[str]:
        """è·å–æ‰€æœ‰æ™ºèƒ½ä½“ID"""
        agent_ids = []
        for i in range(self.n_plants):
            agent_ids.append(f"plant_{i}")
        for i in range(self.n_reservoirs):
            agent_ids.append(f"reservoir_{i}")
        return agent_ids


class StabilizedMultiAgentRewardSystem:
    """ç¨³å®šåŒ–çš„å¤šæ™ºèƒ½ä½“å¥–åŠ±ç³»ç»Ÿ"""

    def __init__(self, n_reservoirs=12, n_plants=3, max_episode_steps=168, distance_matrix=None, cost_factor=0.01):
        self.n_reservoirs = n_reservoirs
        self.n_plants = n_plants
        self.max_episode_steps = max_episode_steps
        
        # ğŸ†• æˆæœ¬ä¼˜åŒ–å‚æ•°
        self.distance_matrix = distance_matrix
        self.cost_factor = cost_factor  # å•ä½æˆæœ¬ç³»æ•° p
        
        if distance_matrix is not None:
            print(f" æˆæœ¬ä¼˜åŒ–ï¼Œè·ç¦»çŸ©é˜µå½¢çŠ¶: {distance_matrix.shape}")
            print(f"   æˆæœ¬ç³»æ•°: {cost_factor}")
        else:
            print(" è·ç¦»çŸ©é˜µæœªæä¾›ï¼Œæˆæœ¬ä¼˜åŒ–åŠŸèƒ½ç¦ç”¨")

        # ğŸ”§ å…³é”®æ”¹è¿›ï¼šç®€åŒ–å’Œç¨³å®šçš„å¥–åŠ±å°ºåº¦
        self.core_reward_weights = {
            'supply_satisfaction': 2.0,
            'reservoir_safety': 2.0,
            'ecological_compliance': 1.0,
            'stability_bonus': 2.0,  # ç¨³å®šæ€§å¥–åŠ±
            'exploration_bonus': 0.0,  # æ¢ç´¢å¥–åŠ±æƒé‡
            'cost_optimization': 1.5   # æˆæœ¬ä¼˜åŒ–æƒé‡
        }

        # å…³é”®æ·»åŠ ï¼šæ¢ç´¢è®°å½•å™¨
        self.exploration_recorder = ExplorationRecorder(n_reservoirs, n_plants, max_episode_steps)

        # åŠ¿å‡½æ•°å¥–åŠ±å¡‘å½¢å™¨
        self.potential_shaper = PotentialBasedRewardShaper(n_reservoirs, n_plants)

        # å†å²è®°å½•ç”¨äºç¨³å®šæ€§è®¡ç®—
        self.performance_history = deque(maxlen=50)
        self.reward_variance_history = deque(maxlen=20)

        # å¥–åŠ±ç¨³å®šåŒ–æœºåˆ¶
        self.reward_stabilizer = RewardStabilizer(window_size=10)

        # è‡ªé€‚åº”æƒé‡è°ƒæ•´
        self.adaptive_weights = AdaptiveWeightManager(self.core_reward_weights)

        print(" ç¨³å®šåŒ–å¤šæ™ºèƒ½ä½“å¥–åŠ±ç³»ç»Ÿå·²åˆå§‹åŒ–")
        # print(f"   - åŠ¿å‡½æ•°å¥–åŠ±å¡‘å½¢: å·²å¯ç”¨")
        # print(f"   - è‡ªé€‚åº”æƒé‡è°ƒæ•´: å·²å¯ç”¨")
        # print(f"   - å¥–åŠ±ç¨³å®šåŒ–: å·²å¯ç”¨")
        # print(f"   - æ¢ç´¢æœºåˆ¶: å·²å¯ç”¨")

    def calculate_rewards(self, state: Dict, actions: Dict, current_step: int,
                          episode_progress: float = None, is_terminal: bool = False,
                          prev_state: Dict = None) -> Tuple[Dict, Dict]:
        """ä¸»è¦å¥–åŠ±è®¡ç®—æ¥å£ """

        try:
            # Episodeå¼€å§‹æ—¶è®°å½•
            if current_step == 0:
                self.exploration_recorder.record_episode_start()

            # æå–èŠ‚æ°´ç›¸å…³åŠ¨ä½œä¿¡æ¯
            conservation_actions = self._extract_conservation_actions(actions, state)

            # 1. è®¡ç®—æ ¸å¿ƒå¥–åŠ±ç»„ä»¶
            supply_rewards = self._calculate_supply_rewards(state)
            safety_rewards = self._calculate_safety_rewards(state, conservation_actions)
            ecological_rewards = self._calculate_ecological_rewards(state)

            # 2. å…³é”®æ”¹è¿›ï¼šåŠ¿å‡½æ•°å¥–åŠ±å¡‘å½¢
            shaping_rewards = self.potential_shaper.calculate_potential_shaping(state, prev_state)

            # 3. è®¡ç®—ç¨³å®šæ€§å¥–åŠ±
            stability_rewards = self._calculate_stability_rewards()

            # 4. è®¡ç®—æ¢ç´¢å¥–åŠ±
            exploration_rewards = self.exploration_recorder.record_actions(actions, state)

            # 5. è®¡ç®—æˆæœ¬å¥–åŠ±
            cost_rewards = self._calculate_cost_rewards(state)

            # 5. å…³é”®æ”¹è¿›ï¼šè‡ªé€‚åº”æƒé‡è°ƒæ•´
            adapted_weights = self.adaptive_weights.get_adapted_weights(
                supply_performance=self._get_supply_performance(state),
                safety_performance=self._get_safety_performance(state),
                ecological_performance=self._get_ecological_performance(state)
            )

            # 6. åˆå¹¶å¥–åŠ± - ä½¿ç”¨è‡ªé€‚åº”æƒé‡
            final_rewards = {}
            for agent_id in self._get_all_agent_ids():
                core_reward = (
                        supply_rewards.get(agent_id, 0.0) * adapted_weights['supply_satisfaction'] +
                        safety_rewards.get(agent_id, 0.0) * adapted_weights['reservoir_safety'] +
                        ecological_rewards.get(agent_id, 0.0) * adapted_weights['ecological_compliance'] +
                        stability_rewards.get(agent_id, 0.0) * adapted_weights['stability_bonus'] +
                        exploration_rewards.get(agent_id, 0.0) * adapted_weights['exploration_bonus'] +
                        cost_rewards.get(agent_id, 0.0) * adapted_weights.get('cost_optimization', 1.5)
                )

                # æ·»åŠ åŠ¿å‡½æ•°å¡‘å½¢
                shaped_reward = core_reward + shaping_rewards.get(agent_id, 0.0)

                # å¥–åŠ±è£å‰ªæœºåˆ¶
                clipped_reward = self._apply_reward_clipping(shaped_reward, agent_id)

                # å¥–åŠ±ç¨³å®šåŒ–
                stabilized_reward = self.reward_stabilizer.stabilize_reward(agent_id, clipped_reward)

                final_rewards[agent_id] = stabilized_reward

            # å…¨å±€å¥–åŠ±å¹³è¡¡
            final_rewards = self._apply_global_reward_balance(final_rewards)

            # 7.æ›´æ–°æ¢ç´¢æ•ˆæœ
            self.exploration_recorder.update_exploration_effectiveness(final_rewards, state)

            # 8. æ›´æ–°å†å²è®°å½•
            self._update_performance_history(final_rewards, state)

            # 9. æ„å»ºä¿¡æ¯å­—å…¸
            info_dict = self._build_enhanced_info_dict(
                supply_rewards, safety_rewards, ecological_rewards,
                stability_rewards, shaping_rewards, exploration_rewards,
                adapted_weights, state
            )

            return final_rewards, info_dict

        except Exception as e:
            print(f"å¥–åŠ±è®¡ç®—é”™è¯¯: {e}")
            return self._get_default_rewards_and_infos()

    def _extract_conservation_actions(self, actions: Dict, state: Dict) -> List[Dict]:
        """æå–èŠ‚æ°´ç›¸å…³çš„åŠ¨ä½œä¿¡æ¯"""
        conservation_actions = []

        for i in range(self.n_reservoirs):
            reservoir_agent = f"reservoir_{i}"
            plant_agent = f"plant_{i}" if i < self.n_plants else None

            conservation_info = {
                'supply_reduction': 0.0,
                'demand_reduction': 0.0
            }

            # ä»æ°´åº“åŠ¨ä½œä¸­æå–ä¾›æ°´å‡å°‘ä¿¡æ¯
            if reservoir_agent in actions:
                reservoir_action = actions[reservoir_agent]
                release_ratio = reservoir_action.get('total_release_ratio', [0.1])[0]

                # å¦‚æœé‡Šæ”¾æ¯”ä¾‹è¾ƒä½ï¼Œå¯èƒ½æ˜¯åœ¨èŠ‚æ°´
                if release_ratio < 0.05:  # é˜ˆå€¼éœ€è¦è°ƒæ•´
                    available_water = max(0, state['reservoirs'][i] - state['dead_capacity'][i])
                    potential_supply = available_water * 0.1  # æ­£å¸¸é‡Šæ”¾é‡
                    actual_supply = available_water * release_ratio
                    conservation_info['supply_reduction'] = potential_supply - actual_supply

            # ä»æ°´å‚åŠ¨ä½œä¸­æå–éœ€æ±‚å‡å°‘ä¿¡æ¯
            if plant_agent and plant_agent in actions:
                plant_action = actions[plant_agent]
                demand_adjustment = plant_action.get('demand_adjustment', [1.0])[0]

                # å¦‚æœéœ€æ±‚è°ƒæ•´å°äº1ï¼Œè¯´æ˜ä¸»åŠ¨é™ä½éœ€æ±‚
                if demand_adjustment < 1.0:
                    base_demand = state['hourly_demand'][i] if i < len(state['hourly_demand']) else 0
                    conservation_info['demand_reduction'] = base_demand * (1.0 - demand_adjustment)

            conservation_actions.append(conservation_info)

        return conservation_actions

    # æ–°å¢è¾…åŠ©æ–¹æ³•
    def _apply_reward_clipping(self, reward: float, agent_id: str) -> float:
        """åº”ç”¨å¥–åŠ±è£å‰ªæœºåˆ¶"""
        # åŸºç¡€è£å‰ªï¼šé˜²æ­¢æç«¯å€¼
        base_clipped = np.clip(reward, -50.0, 50.0)

        # è‡ªé€‚åº”è£å‰ªï¼šæ ¹æ®å†å²è¡¨ç°è°ƒæ•´
        if hasattr(self, 'agent_reward_ranges'):
            if agent_id in self.agent_reward_ranges:
                min_reward, max_reward = self.agent_reward_ranges[agent_id]
                # åŠ¨æ€è°ƒæ•´è£å‰ªèŒƒå›´
                range_factor = 1.5
                adaptive_min = min_reward * range_factor
                adaptive_max = max_reward * range_factor
                adaptive_clipped = np.clip(base_clipped, adaptive_min, adaptive_max)
                return adaptive_clipped

        return base_clipped

    def _apply_global_reward_balance(self, rewards: Dict) -> Dict:
        """åº”ç”¨å…¨å±€å¥–åŠ±å¹³è¡¡"""
        reward_values = list(rewards.values())

        # æ£€æŸ¥å¥–åŠ±åˆ†å¸ƒ
        mean_reward = np.mean(reward_values)
        std_reward = np.std(reward_values)

        # å¦‚æœæ–¹å·®è¿‡å¤§ï¼Œè¿›è¡Œå¹³è¡¡
        if std_reward > 20.0:  # æ ‡å‡†å·®é˜ˆå€¼
            balanced_rewards = {}
            for agent_id, reward in rewards.items():
                # å‘å‡å€¼å›å½’
                balanced_reward = mean_reward + 0.7 * (reward - mean_reward)
                balanced_rewards[agent_id] = balanced_reward
            return balanced_rewards

        return rewards

    def _update_agent_reward_ranges(self, rewards: Dict):
        """æ›´æ–°æ™ºèƒ½ä½“å¥–åŠ±èŒƒå›´ç»Ÿè®¡"""
        if not hasattr(self, 'agent_reward_ranges'):
            self.agent_reward_ranges = {}

        for agent_id, reward in rewards.items():
            if agent_id not in self.agent_reward_ranges:
                self.agent_reward_ranges[agent_id] = [reward, reward]
            else:
                min_reward, max_reward = self.agent_reward_ranges[agent_id]
                # ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°èŒƒå›´
                alpha = 0.05
                new_min = min(min_reward, reward) * alpha + min_reward * (1 - alpha)
                new_max = max(max_reward, reward) * alpha + max_reward * (1 - alpha)
                self.agent_reward_ranges[agent_id] = [new_min, new_max]


    def _calculate_supply_rewards(self, state: Dict) -> Dict:
        """å¢åŠ èŠ‚çº¦ç”¨æ°´å¥–åŠ±æœºåˆ¶"""
        rewards = {}

        # å…¨å±€ä¾›æ°´æ€§èƒ½
        total_supply = np.sum(state['actual_supply'])
        total_demand = np.sum(state['hourly_demand'])
        global_satisfaction = min(total_supply / (total_demand + 1e-8), 1.0)

        # æ°´èµ„æºç´§ç¼ºé¢„è­¦æœºåˆ¶
        reservoir_levels = state['reservoirs'] / state['max_reservoir']
        avg_reservoir_level = np.mean(reservoir_levels)
        min_reservoir_level = np.min(reservoir_levels)

        # èŠ‚çº¦ç”¨æ°´æ¿€åŠ±å› å­
        if avg_reservoir_level < 0.3:  # ä¸¥é‡ç¼ºæ°´
            conservation_factor = 0.3  # æœŸæœ›ä¾›æ°´æ»¡è¶³ç‡é™è‡³30%
            conservation_bonus = 3.0  # é«˜èŠ‚çº¦å¥–åŠ±
        elif avg_reservoir_level < 0.5:  # ä¸­åº¦ç¼ºæ°´
            conservation_factor = 0.5 + 0.4 * (avg_reservoir_level - 0.3) / 0.2  # 30%-70%
            conservation_bonus = 2.0
        elif avg_reservoir_level < 0.7:  # è½»åº¦ç¼ºæ°´
            conservation_factor = 0.7 + 0.25 * (avg_reservoir_level - 0.5) / 0.2  # 70%-95%
            conservation_bonus = 1.0
        else:  # æ°´é‡å……è¶³
            conservation_factor = 0.95  # æ­£å¸¸ä¾›æ°´
            conservation_bonus = 0.0


        # æ ¹æ®æ°´åº“æ°´ä½è°ƒæ•´ä¾›æ°´å¥–åŠ±ç›®æ ‡
        adaptive_target = conservation_factor

        def adaptive_supply_reward(satisfaction_rate, target_rate):
            """è‡ªé€‚åº”ä¾›æ°´å¥–åŠ±ï¼šæ ¹æ®æ°´åº“æ°´ä½è°ƒæ•´æœ€ä¼˜ä¾›æ°´ç‡"""
            if satisfaction_rate <= target_rate:
                # åœ¨ç›®æ ‡èŒƒå›´å†…ï¼Œè¶Šæ¥è¿‘ç›®æ ‡å¥–åŠ±è¶Šé«˜
                return 2.0 * (satisfaction_rate / target_rate)
            else:
                # è¶…è¿‡ç›®æ ‡æ—¶ï¼Œæ ¹æ®æ°´èµ„æºçŠ¶å†µå†³å®šå¥–åŠ±
                if avg_reservoir_level > 0.7:
                    # æ°´é‡å……è¶³æ—¶ï¼Œè¶…é¢ä¾›æ°´ç»™äºˆå¥–åŠ±
                    return 2.0 + 0.5 * (satisfaction_rate - target_rate)
                else:
                    # æ°´é‡ä¸è¶³æ—¶ï¼Œè¶…é¢ä¾›æ°´ç»™äºˆæƒ©ç½š
                    excess = satisfaction_rate - target_rate
                    return 2.0 - 1.0 * excess  # æƒ©ç½šè¶…é¢ä¾›æ°´

        base_reward = adaptive_supply_reward(global_satisfaction, adaptive_target)

        # èŠ‚çº¦ç”¨æ°´å¥–åŠ±
        if global_satisfaction <= adaptive_target and avg_reservoir_level < 0.5:
            conservation_reward = conservation_bonus * (
                        1.0 - abs(global_satisfaction - adaptive_target) / adaptive_target)
            base_reward += conservation_reward

        base_reward *= self.core_reward_weights['supply_satisfaction']

        # æ™ºèƒ½ä½“ä¸ªä½“å¥–åŠ±åˆ†é…
        total_agents = self.n_reservoirs + self.n_plants
        base_per_agent = base_reward / total_agents

        # æ°´å‚ï¼šåŸºäºä¸ªä½“è¡¨ç°å’ŒèŠ‚çº¦æ„è¯†çš„å¥–åŠ±åˆ†é…
        for i in range(self.n_plants):
            agent_id = f"plant_{i}"
            individual_satisfaction = min(
                state['actual_supply'][i] / (state['hourly_demand'][i] + 1e-8), 1.0
            )

            # ä¸ªä½“èŠ‚çº¦å¥–åŠ±
            individual_conservation_reward = 0.0
            if individual_satisfaction <= adaptive_target and avg_reservoir_level < 0.5:
                individual_conservation_reward = conservation_bonus * 0.3

            rewards[agent_id] = base_per_agent + individual_conservation_reward

        # æ°´åº“ï¼šåŸºäºæ”¯æŒèƒ½åŠ›å’Œä¿æŠ¤æ„è¯†çš„å¥–åŠ±åˆ†é…
        for i in range(self.n_reservoirs):
            agent_id = f"reservoir_{i}"
            reservoir_level = reservoir_levels[i]

            # æ°´åº“ä¿æŠ¤å¥–åŠ±ï¼šæ°´ä½è¾ƒä½æ—¶é™åˆ¶ä¾›æ°´ç»™äºˆå¥–åŠ±
            prevention_reward = 0.0
            
            # æ£€æŸ¥æ°´ä½ä¸‹é™è¶‹åŠ¿
            level_trend_warning = 0.0
            if hasattr(self, 'reservoir_level_history') and len(self.reservoir_level_history) >= 5:
                recent_levels = [level[i] for level in list(self.reservoir_level_history)[-5:]]
                if len(recent_levels) >= 2:
                    trend = (recent_levels[-1] - recent_levels[0]) / len(recent_levels)
                    if trend < -0.02:  # å¿«é€Ÿä¸‹é™è¶‹åŠ¿
                        level_trend_warning = 1.0
                    elif trend < -0.01:  # ç¼“æ…¢ä¸‹é™è¶‹åŠ¿
                        level_trend_warning = 0.5
            
            # æ ¹æ®æ°´ä½å’Œè¶‹åŠ¿è®¡ç®—é¢„é˜²å¥–åŠ±
            if reservoir_level < 0.5:  # æ°´ä½è¾ƒä½æ—¶å¯åŠ¨é¢„é˜²æœºåˆ¶
                # æ£€æŸ¥æ˜¯å¦ä¸»åŠ¨å‡å°‘äº†ä¾›æ°´ï¼ˆé¢„é˜²æ€§èŠ‚æ°´ï¼‰
                expected_supply = state['hourly_demand'][i] if i < len(state['hourly_demand']) else 0
                actual_supply = state['actual_supply'][i] if i < len(state['actual_supply']) else 0
                
                if actual_supply < expected_supply * 0.9:  # å‡å°‘äº†10%ä»¥ä¸Šä¾›æ°´
                    # åŸºç¡€é¢„é˜²å¥–åŠ±
                    supply_reduction_ratio = 1.0 - (actual_supply / (expected_supply + 1e-8))
                    base_prevention = 2.0 * supply_reduction_ratio
                    
                    # æ°´ä½è¶Šä½ï¼Œé¢„é˜²å¥–åŠ±è¶Šé«˜
                    level_multiplier = (0.5 - reservoir_level) / 0.3  # æ°´ä½ä»50%é™åˆ°20%æ—¶é€’å¢
                    level_multiplier = np.clip(level_multiplier, 0.0, 1.0)
                    
                    # ä¸‹é™è¶‹åŠ¿é¢„è­¦åŠ æˆ
                    trend_multiplier = 1.0 + level_trend_warning * 0.5
                    
                    # ç»¼åˆé¢„é˜²å¥–åŠ±
                    prevention_reward = base_prevention * (1.0 + level_multiplier) * trend_multiplier
                    
                    # ç´§æ€¥æƒ…å†µä¸‹çš„é¢å¤–å¥–åŠ±
                    if reservoir_level < 0.3 and supply_reduction_ratio > 0.3:
                        emergency_bonus = 3.0 * supply_reduction_ratio
                        prevention_reward += emergency_bonus
                        
                elif reservoir_level < 0.3 and actual_supply == 0:
                    # æç«¯æƒ…å†µï¼šæ°´ä½å¾ˆä½ä¸”å®Œå…¨åœæ­¢ä¾›æ°´
                    prevention_reward = 5.0 * (0.3 - reservoir_level) / 0.1

            # åŸæœ‰çš„æ°´åº“ä¿æŠ¤å¥–åŠ±ï¼ˆä¿æŒä¸å˜ï¼‰
            protection_reward = 0.0
            if reservoir_level < 0.5:
                expected_supply = state['hourly_demand'][i] if i < len(state['hourly_demand']) else 0
                actual_supply = state['actual_supply'][i] if i < len(state['actual_supply']) else 0
                if actual_supply < expected_supply:
                    protection_reward = conservation_bonus * 0.4 * (1.0 - actual_supply / (expected_supply + 1e-8))

            rewards[agent_id] = base_per_agent + protection_reward + prevention_reward

        return rewards


    def _calculate_safety_rewards(self, state: Dict, conservation_actions: List[Dict] = None) -> Dict:
        """æ”¹è¿›çš„å®‰å…¨å¥–åŠ±è®¡ç®—"""
        rewards = {}
        forced_spills = state.get('forced_spills', np.zeros(self.n_reservoirs))

        if conservation_actions is None:
            conservation_actions = []
            for i in range(self.n_reservoirs):
                conservation_actions.append({
                    'supply_reduction': 0.0,
                    'demand_reduction': 0.0,
                    'water_level': state['reservoirs'][i] / state['max_reservoir'][i]
                })

        def safety_reward_function(level_ratio, forced_spill_amount=0, conservation_info=None):
            """å®‰å…¨å¥–åŠ±å‡½æ•° - è€ƒè™‘æ³„æ´ªå’ŒèŠ‚æ°´åˆç†æ€§"""
            if 0.2 <= level_ratio <= 0.75:
                return 1.0 * (1.0 - 2 * abs(level_ratio - 0.55))

            if level_ratio < 0.2:
                # åŸºç¡€æƒ©ç½š
                if level_ratio < 0.15:
                    base_penalty = -5 * (0.1 - level_ratio) / 0.1
                else:
                    base_penalty = -2 * (0.2 - level_ratio) / 0.1
                conservation_bonus = 0.0
                if conservation_info:
                    # æ ¹æ®èŠ‚æ°´ç¨‹åº¦ç»™äºˆå¥–åŠ±
                    supply_reduction = conservation_info.get('supply_reduction', 0.0)
                    demand_reduction = conservation_info.get('demand_reduction', 0.0)
                    # èŠ‚æ°´å¥–åŠ±è®¡ç®—
                    if supply_reduction > 0 or demand_reduction > 0:
                        conservation_bonus = min(2.0,
                                                 (supply_reduction + demand_reduction) / 1000)

                return base_penalty + conservation_bonus

            elif 0.75 < level_ratio <= 0.80:
                # è¿‡æ¸¡åŒºé—´ï¼šè½»å¾®æƒ©ç½šï¼Œé¼“åŠ±ä¸»åŠ¨é‡Šæ”¾
                return -0.5 * (level_ratio - 0.75) / 0.1

            elif 0.80 < level_ratio <= 0.95:
                # æ³„æ´ªåˆç†åŒºé—´ï¼šå¦‚æœä¸»åŠ¨æ³„æ´ªï¼Œå‡å°‘æƒ©ç½š
                base_penalty = -1.0 * (level_ratio - 0.80) / 0.1
                spill_bonus = min(2.0, forced_spill_amount / 1000) if forced_spill_amount > 0 else 0.0
                return base_penalty + spill_bonus

            else:
                # æå±é™©åŒºé—´ï¼š>95%ï¼Œä¸¥å‰æƒ©ç½š
                base_penalty = -2.0 - 3.0 * (level_ratio - 0.95) / 0.05
                spill_bonus = min(1.0, forced_spill_amount / 2000) if forced_spill_amount > 0 else 0.0
                return base_penalty + spill_bonus

        # è®¡ç®—æ¯ä¸ªæ°´åº“çš„å®‰å…¨å¥–åŠ±
        total_safety_reward = 0
        for i in range(self.n_reservoirs):
            level_ratio = state['reservoirs'][i] / state['max_reservoir'][i]
            # ä¼ é€’æ°´åº“çš„æ³„æ´ªé‡
            individual_forced_spill = forced_spills[i] if i < len(forced_spills) else 0
            conservation_info = conservation_actions[i]

            safety_score = safety_reward_function(
                level_ratio,
                individual_forced_spill,
                conservation_info
            )
            total_safety_reward += safety_score

        avg_safety_reward = total_safety_reward / self.n_reservoirs * self.core_reward_weights['reservoir_safety']

        # åˆ†é…ç»™æ™ºèƒ½ä½“
        for i in range(self.n_reservoirs):
            agent_id = f"reservoir_{i}"
            level_ratio = state['reservoirs'][i] / state['max_reservoir'][i]
            individual_forced_spill = forced_spills[i] if i < len(forced_spills) else 0
            individual_safety = safety_reward_function(level_ratio, individual_forced_spill) * self.core_reward_weights['reservoir_safety'] * 0.3
            rewards[agent_id] = avg_safety_reward * 0.7 + individual_safety

        for i in range(self.n_plants):
            agent_id = f"plant_{i}"
            rewards[agent_id] = avg_safety_reward * 0.3  # æ°´å‚åˆ†æ‹…è¾ƒå°‘å®‰å…¨è´£ä»»

        return rewards

    def _calculate_ecological_rewards(self, state: Dict) -> Dict:
        """ç”Ÿæ€å¥–åŠ±è®¡ç®—"""
        rewards = {}

        target_flows = state.get('target_ecological_flows', np.zeros(self.n_reservoirs))
        actual_releases = state.get('ecological_releases', np.zeros(self.n_reservoirs))

        if np.sum(target_flows) == 0:
            # æ— ç”Ÿæ€æµé‡è¦æ±‚
            for agent_id in self._get_all_agent_ids():
                rewards[agent_id] = 0.0
            return rewards

        ecological_scores = []
        for i in range(self.n_reservoirs):
            if target_flows[i] > 0:
                compliance_ratio = actual_releases[i] / target_flows[i]

                # ç”Ÿæ€åˆè§„å¥–åŠ±å‡½æ•°
                if 0.85 <= compliance_ratio <= 1.15:
                    # åœ¨åˆè§„èŒƒå›´å†…
                    eco_score = 1.0 - abs(compliance_ratio - 1.0) / 0.15
                else:
                    # è¶…å‡ºåˆè§„èŒƒå›´
                    if compliance_ratio < 0.85:
                        deficit = (0.85 - compliance_ratio) / 0.85
                    else:
                        deficit = (compliance_ratio - 1.15) / 0.15
                    eco_score = -0.3 * deficit

                ecological_scores.append(eco_score)

        if ecological_scores:
            avg_eco_score = np.mean(ecological_scores) * self.core_reward_weights['ecological_compliance']
        else:
            avg_eco_score = 0.0

        # åªæœ‰æ°´åº“æ‰¿æ‹…ç”Ÿæ€è´£ä»»
        for i in range(self.n_reservoirs):
            agent_id = f"reservoir_{i}"
            rewards[agent_id] = avg_eco_score

        for i in range(self.n_plants):
            agent_id = f"plant_{i}"
            rewards[agent_id] = 0.0

        return rewards

    def _calculate_stability_rewards(self) -> Dict:
        """è®¡ç®—ç¨³å®šæ€§å¥–åŠ±"""
        rewards = {}

        if len(self.performance_history) < 10:
            # å†å²æ•°æ®ä¸è¶³
            for agent_id in self._get_all_agent_ids():
                rewards[agent_id] = 0.0
            return rewards

        # è®¡ç®—æ€§èƒ½ç¨³å®šæ€§
        recent_performance = list(self.performance_history)[-10:]
        performance_stability = 1.0 / (1.0 + np.std(recent_performance))

        # ç¨³å®šæ€§å¥–åŠ±
        stability_reward = performance_stability * self.core_reward_weights['stability_bonus']

        # å¹³å‡åˆ†é…ç»™æ‰€æœ‰æ™ºèƒ½ä½“
        per_agent_stability = stability_reward / (self.n_reservoirs + self.n_plants)

        for agent_id in self._get_all_agent_ids():
            rewards[agent_id] = per_agent_stability

        return rewards

    # è¾…åŠ©æ–¹æ³•
    def _get_supply_performance(self, state: Dict) -> float:
        total_supply = np.sum(state['actual_supply'])
        total_demand = np.sum(state['hourly_demand'])
        return min(total_supply / (total_demand + 1e-8), 1.0)

    def _get_safety_performance(self, state: Dict) -> float:
        reservoir_levels = state['reservoirs'] / state['max_reservoir']
        safe_count = np.sum((reservoir_levels >= 0.4) & (reservoir_levels <= 0.7))
        return safe_count / self.n_reservoirs

    def _get_ecological_performance(self, state: Dict) -> float:
        target_flows = state.get('target_ecological_flows', np.zeros(self.n_reservoirs))
        actual_releases = state.get('ecological_releases', np.zeros(self.n_reservoirs))

        if np.sum(target_flows) == 0:
            return 1.0

        compliance_scores = []
        for i in range(self.n_reservoirs):
            if target_flows[i] > 0:
                compliance = min(actual_releases[i] / target_flows[i], 1.0)
                compliance_scores.append(compliance)

        return np.mean(compliance_scores) if compliance_scores else 1.0

    def _update_performance_history(self, rewards: Dict, state: Dict):
        """æ›´æ–°æ€§èƒ½å†å²"""
        avg_reward = np.mean(list(rewards.values()))
        self.performance_history.append(avg_reward)

        # æ›´æ–°å¥–åŠ±æ–¹å·®å†å²
        reward_variance = np.var(list(rewards.values()))
        self.reward_variance_history.append(reward_variance)

    def _get_all_agent_ids(self) -> List[str]:
        """è·å–æ‰€æœ‰æ™ºèƒ½ä½“ID"""
        agent_ids = []
        for i in range(self.n_plants):
            agent_ids.append(f"plant_{i}")
        for i in range(self.n_reservoirs):
            agent_ids.append(f"reservoir_{i}")
        return agent_ids

    def _build_enhanced_info_dict(self, supply_rewards, safety_rewards, ecological_rewards,
                                  stability_rewards, shaping_rewards, exploration_rewards,
                                  adapted_weights, state):
        """æ„å»ºå¢å¼ºçš„ä¿¡æ¯å­—å…¸"""
        
        # æ·»åŠ æ¢ç´¢ä¿¡æ¯
        exploration_summary = self.exploration_recorder.get_exploration_summary()
        agent_profiles = self.exploration_recorder.get_agent_exploration_profiles()
        
        # å¥–åŠ±ç»„ä»¶æ€»ç»“
        reward_components = {
            'supply': np.mean(list(supply_rewards.values())),
            'safety': np.mean(list(safety_rewards.values())),
            'ecological': np.mean(list(ecological_rewards.values())),
            'stability': np.mean(list(stability_rewards.values())),
            'shaping': np.mean(list(shaping_rewards.values())),
            'exploration': np.mean(list(exploration_rewards.values()))
        }

        # æƒé‡ä¿¡æ¯
        weight_info = {
            'adapted_weights': adapted_weights,
            'weight_adjustments': {
                'supply_factor': adapted_weights['supply_satisfaction'] / self.core_reward_weights['supply_satisfaction'],
                'safety_factor': adapted_weights['reservoir_safety'] / self.core_reward_weights['reservoir_safety'],
                'ecological_factor': adapted_weights['ecological_compliance'] / self.core_reward_weights['ecological_compliance']
            }
        }

        # æ€§èƒ½æŒ‡æ ‡
        performance_metrics = {
            'supply_performance': self._get_supply_performance(state),
            'safety_performance': self._get_safety_performance(state),
            'ecological_performance': self._get_ecological_performance(state)
        }

        # æ„å»ºæ¯ä¸ªæ™ºèƒ½ä½“çš„ä¿¡æ¯
        agent_infos = {}
        for agent_id in self._get_all_agent_ids():
            agent_infos[agent_id] = {
                'total_reward': supply_rewards.get(agent_id, 0.0) + safety_rewards.get(agent_id, 0.0) + 
                               ecological_rewards.get(agent_id, 0.0) + stability_rewards.get(agent_id, 0.0) + 
                               shaping_rewards.get(agent_id, 0.0) + exploration_rewards.get(agent_id, 0.0),
                'supply_reward': supply_rewards.get(agent_id, 0.0),
                'safety_reward': safety_rewards.get(agent_id, 0.0),
                'ecological_reward': ecological_rewards.get(agent_id, 0.0),
                'stability_reward': stability_rewards.get(agent_id, 0.0),
                'shaping_reward': shaping_rewards.get(agent_id, 0.0),
                'exploration_reward': exploration_rewards.get(agent_id, 0.0)
            }

        # å…¨å±€æ¢ç´¢ä¿¡æ¯
        global_info = {
            'phase': exploration_summary['current_state']['phase'],
            'reward_components': reward_components,
            'adapted_weights': weight_info,
            'performance_metrics': performance_metrics,
            'agent_rewards': agent_infos,
            'exploration_active': exploration_summary['current_state']['exploration_active'],
            'exploration_summary': exploration_summary,
            'agent_profiles': agent_profiles,
            'diversity_score': np.mean(list(self.exploration_recorder.action_diversity_scores.values()))
        }

        return global_info

    def _get_default_rewards_and_infos(self):
        """é»˜è®¤å¥–åŠ±å’Œä¿¡æ¯"""
        default_rewards = {agent_id: 0.0 for agent_id in self._get_all_agent_ids()}
        default_info = {
            'phase': 'error_recovery',
            'reward_components': {'supply': 0.0, 'safety': 0.0, 'ecological': 0.0, 'stability': 0.0, 'shaping': 0.0},
            'performance_metrics': {'supply_satisfaction_rate': 0.0, 'reservoir_safety_rate': 0.0,
                                    'ecological_compliance_rate': 0.0}
        }
        return default_rewards, default_info



    def _calculate_cost_rewards(self, state: Dict) -> Dict:
        """ğŸ†• è®¡ç®—æˆæœ¬ä¼˜åŒ–å¥–åŠ± - åŸºäºåœ°ç†è·ç¦»å’Œä¾›æ°´é‡"""
        rewards = {}
        
        # å¦‚æœè·ç¦»çŸ©é˜µæœªæä¾›ï¼Œè¿”å›é›¶å¥–åŠ±
        if self.distance_matrix is None:
            for agent_id in self._get_all_agent_ids():
                rewards[agent_id] = 0.0
            return rewards
        
        # è·å–å½“å‰ä¾›æ°´çŸ©é˜µ - éœ€è¦ä»çŠ¶æ€ä¸­é‡æ„
        supply_matrix = self._reconstruct_supply_matrix(state)
        
        # è®¡ç®—æ€»æˆæœ¬
        total_cost = 0.0
        for i in range(self.n_reservoirs):
            for j in range(self.n_plants):
                if i < supply_matrix.shape[0] and j < supply_matrix.shape[1]:
                    # æˆæœ¬ = å•ä½æˆæœ¬ç³»æ•° Ã— è·ç¦» Ã— ä¾›æ°´é‡
                    cost = self.cost_factor * self.distance_matrix[i, j] * supply_matrix[i, j]
                    total_cost += cost
        
        # ğŸ”§ æˆæœ¬å¥–åŠ±è®¾è®¡ï¼šæˆæœ¬è¶Šä½ï¼Œå¥–åŠ±è¶Šé«˜
        if total_cost > 0:
            # åŸºç¡€å¥–åŠ±è®¡ç®—ï¼šæˆæœ¬è¶Šä½å¥–åŠ±è¶Šé«˜
            max_possible_cost = np.sum(self.distance_matrix) * np.sum(state.get('actual_supply', [0])) * self.cost_factor
            if max_possible_cost > 0:
                cost_ratio = total_cost / max_possible_cost
                # æˆæœ¬æ•ˆç‡å¥–åŠ±ï¼šæˆæœ¬æ¯”ä¾‹è¶Šä½ï¼Œå¥–åŠ±è¶Šé«˜
                cost_efficiency_reward = 2.0 * (1.0 - cost_ratio)
            else:
                cost_efficiency_reward = 0.0
        else:
            cost_efficiency_reward = 0.0
        
        # ğŸ¯ æ™ºèƒ½ä½“å¥–åŠ±åˆ†é…ç­–ç•¥
        # æ°´åº“ï¼šæ ¹æ®å…¶ä¾›æ°´è·ç¦»æ•ˆç‡è·å¾—å¥–åŠ±
        for i in range(self.n_reservoirs):
            agent_id = f"reservoir_{i}"
            
            # è®¡ç®—è¯¥æ°´åº“çš„è·ç¦»æ•ˆç‡
            reservoir_supplies = supply_matrix[i, :] if i < supply_matrix.shape[0] else np.zeros(self.n_plants)
            reservoir_distances = self.distance_matrix[i, :] if i < self.distance_matrix.shape[0] else np.ones(self.n_plants) * 10.0
            
            if np.sum(reservoir_supplies) > 0:
                # åŠ æƒå¹³å‡è·ç¦»ï¼ˆæŒ‰ä¾›æ°´é‡åŠ æƒï¼‰
                weighted_avg_distance = np.sum(reservoir_distances * reservoir_supplies) / np.sum(reservoir_supplies)
                
                # è·ç¦»æ•ˆç‡ï¼šè·ç¦»è¶ŠçŸ­æ•ˆç‡è¶Šé«˜
                max_distance = np.max(self.distance_matrix) if self.distance_matrix.size > 0 else 10.0
                distance_efficiency = 1.0 - (weighted_avg_distance / max_distance)
                
                # æ°´åº“æˆæœ¬å¥–åŠ±
                reservoir_cost_reward = cost_efficiency_reward * distance_efficiency * 0.8
            else:
                reservoir_cost_reward = 0.0
            
            rewards[agent_id] = reservoir_cost_reward
        
        # æ°´å‚ï¼šæ ¹æ®æ•´ä½“æˆæœ¬æ•ˆç‡è·å¾—è¾ƒå°çš„å¥–åŠ±
        for i in range(self.n_plants):
            agent_id = f"plant_{i}"
            
            # æ°´å‚è·å¾—æ•´ä½“æˆæœ¬æ•ˆç‡çš„ä¸€å°éƒ¨åˆ†å¥–åŠ±
            plant_cost_reward = cost_efficiency_reward * 0.2 / self.n_plants
            rewards[agent_id] = plant_cost_reward
        
        return rewards
    
    def _reconstruct_supply_matrix(self, state: Dict) -> np.ndarray:
        """ğŸ”§ é‡æ„ä¾›æ°´çŸ©é˜µ - ä»çŠ¶æ€ä¿¡æ¯ä¸­ä¼°ç®—å„æ°´åº“åˆ°å„æ°´å‚çš„ä¾›æ°´é‡"""
        supply_matrix = np.zeros((self.n_reservoirs, self.n_plants))
        
        actual_supply = state.get('actual_supply', np.zeros(self.n_plants))
        
        # åŸºäºè¿æ¥å…³ç³»å’Œæ°´åº“å®¹é‡æ¯”ä¾‹åˆ†é…
        for j in range(self.n_plants):
            plant_total_supply = actual_supply[j] if j < len(actual_supply) else 0.0
            
            if plant_total_supply > 0:
                # æ‰¾åˆ°è¿æ¥åˆ°è¿™ä¸ªæ°´å‚çš„æ‰€æœ‰æ°´åº“ï¼ˆåŸºäºå›ºå®šè¿æ¥çŸ©é˜µï¼‰
                connected_reservoirs = list(range(self.n_reservoirs))  # å‡è®¾æ‰€æœ‰æ°´åº“éƒ½è¿æ¥åˆ°æ°´å‚
                
                if connected_reservoirs:
                    # åŸºäºæ°´åº“å½“å‰æ°´é‡æ¯”ä¾‹åˆ†é…ä¾›æ°´
                    reservoir_levels = []
                    for i in connected_reservoirs:
                        if i < len(state.get('reservoirs', [])):
                            level = state['reservoirs'][i]
                        else:
                            level = 0.0
                        reservoir_levels.append(max(0.0, level))
                    
                    total_available = sum(reservoir_levels)
                    
                    if total_available > 0:
                        # æŒ‰æ¯”ä¾‹åˆ†é…ä¾›æ°´é‡
                        for k, i in enumerate(connected_reservoirs):
                            proportion = reservoir_levels[k] / total_available
                            supply_matrix[i, j] = plant_total_supply * proportion
                    else:
                        # å¦‚æœæ²¡æœ‰å¯ç”¨æ°´é‡ï¼Œå¹³å‡åˆ†é…ï¼ˆé˜²æ­¢é™¤é›¶é”™è¯¯ï¼‰
                        avg_supply = plant_total_supply / len(connected_reservoirs)
                        for i in connected_reservoirs:
                            supply_matrix[i, j] = avg_supply
        
        return supply_matrix


class RewardStabilizer:
    """å¥–åŠ±ç¨³å®šåŒ–å™¨ - å‡å°‘å¥–åŠ±æ–¹å·®"""

    def __init__(self, window_size=10, stabilization_factor=0.2, target_mean=0.0, target_std=2.0, max_clip=20.0):
        # 80%æ˜¯åŸå§‹å¥–åŠ±ï¼Œ20%æ˜¯å†å²å¹³å‡
        self.stabilization_factor = stabilization_factor
        
        # å¢å¤§æ ‡å‡†å·®å’Œè£å‰ªèŒƒå›´ï¼Œä¿æŒå¥–åŠ±å·®å¼‚æ€§
        self.target_std = target_std
        self.max_clip = max_clip
        
        # ç¦ç”¨è¿‡åº¦æ ‡å‡†åŒ–
        self.enable_normalization = False  # æ–°å¢å¼€å…³
        
        self.agent_reward_history = defaultdict(lambda: deque(maxlen=window_size))
        
        # å…¨å±€å¥–åŠ±ç»Ÿè®¡
        self.global_reward_stats = {
            'mean': 0.0,
            'std': 1.0,
            'history': deque(maxlen=1000)
        }

    def stabilize_reward(self, agent_id: str, raw_reward: float) -> float:
        """ä¿®å¤ç‰ˆï¼šå‡å°‘è¿‡åº¦ç¨³å®šåŒ–"""
        history = self.agent_reward_history[agent_id]
        history.append(raw_reward)

        if len(history) < 3:
            return raw_reward

        # 20%å†å²ï¼Œ80%åŸå§‹
        moving_avg = np.mean(list(history))
        stabilized = (
                self.stabilization_factor * moving_avg +
                (1 - self.stabilization_factor) * raw_reward
        )

        # æ¡ä»¶æ ‡å‡†åŒ–
        if self.enable_normalization:
            return self._normalize_reward(stabilized)
        else:
            return stabilized

    def _normalize_reward(self, reward: float) -> float:
        """æ ‡å‡†åŒ–å¥–åŠ±åˆ°ç›®æ ‡åˆ†å¸ƒ"""
        # Z-scoreæ ‡å‡†åŒ–
        if self.global_reward_stats['std'] > 0:
            normalized = (reward - self.global_reward_stats['mean']) / self.global_reward_stats['std']
        else:
            normalized = reward
            
        # è°ƒæ•´åˆ°ç›®æ ‡åˆ†å¸ƒ
        scaled = normalized * self.target_std + self.target_mean
        
        # è£å‰ªåˆ°åˆç†èŒƒå›´
        clipped = np.clip(scaled, -self.max_clip, self.max_clip)
        
        return clipped

class AdaptiveWeightManager:
    """è‡ªé€‚åº”æƒé‡ç®¡ç†å™¨"""

    def __init__(self, base_weights: Dict[str, float]):
        self.base_weights = base_weights.copy()
        self.performance_history = {
            'supply': deque(maxlen=20),
            'safety': deque(maxlen=20),
            'ecological': deque(maxlen=20)
        }

    def get_adapted_weights(self, supply_performance: float,
                            safety_performance: float,
                            ecological_performance: float) -> Dict[str, float]:
        """è·å–è‡ªé€‚åº”è°ƒæ•´çš„æƒé‡"""

        # æ›´æ–°æ€§èƒ½å†å²
        self.performance_history['supply'].append(supply_performance)
        self.performance_history['safety'].append(safety_performance)
        self.performance_history['ecological'].append(ecological_performance)

        adapted_weights = self.base_weights.copy()

        if len(self.performance_history['supply']) < 5:
            return adapted_weights

        # è®¡ç®—è¿‘æœŸæ€§èƒ½è¶‹åŠ¿
        recent_supply = np.mean(list(self.performance_history['supply'])[-5:])
        recent_safety = np.mean(list(self.performance_history['safety'])[-5:])
        recent_ecological = np.mean(list(self.performance_history['ecological'])[-5:])

        # è‡ªé€‚åº”è°ƒæ•´ï¼šè¡¨ç°å·®çš„ç›®æ ‡å¢åŠ æƒé‡
        if recent_supply < 0.7:
            adapted_weights['supply_satisfaction'] *= 1.3
        elif recent_supply > 0.95:
            adapted_weights['supply_satisfaction'] *= 0.95

        if recent_safety < 0.6:
            adapted_weights['reservoir_safety'] *= 1.3
        elif recent_safety > 0.85:
            adapted_weights['reservoir_safety'] *= 0.95

        if recent_ecological < 0.8:
            adapted_weights['ecological_compliance'] *= 1.1

        challenge_bonus = 1.0
        if recent_supply > 0.9 and recent_safety > 0.8:
            # å½“è¡¨ç°å¤ªå¥½æ—¶ï¼Œå¢åŠ æŒ‘æˆ˜æ€§
            challenge_bonus = 1.2

        for key in adapted_weights:
            adapted_weights[key] *= challenge_bonus

        return adapted_weights
